{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.impute import SimpleImputer\n",
    "from category_encoders import MEstimateEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from pathlib import Path\n",
    "from scipy.stats import skew \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import warnings\n",
    "import gc\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.linear_model import ElasticNet, LinearRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "\n",
    "    df['Episode_Num'] = df['Episode_Title'].str.extract(r'(\\d+)').astype(float)\n",
    "    df = df.drop('Episode_Title', axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def encode(df):\n",
    "    # The nominative (unordered) categorical features\n",
    "    features_nom = [\n",
    "    'Podcast_Name',\n",
    "    'Genre',\n",
    "    'Publication_Day',\n",
    "    ]\n",
    "    \n",
    "    # The ordinal (ordered) categorical features\n",
    "    ordered_levels = {\n",
    "        'Episode_Sentiment': ['Negative', 'Neutral', 'Positive'],\n",
    "        'Publication_Time': ['Morning', 'Afternoon', 'Evening', 'Night']\n",
    "    }\n",
    "\n",
    "    ordered_levels = {key: [\"None\"] + value for key, value in\n",
    "                  ordered_levels.items()}\n",
    "\n",
    "    \n",
    "    # Nominal categories\n",
    "    for name in features_nom:\n",
    "        df[name] = df[name].astype(\"category\")\n",
    "        if \"None\" not in df[name].cat.categories:\n",
    "            df[name] = df[name].cat.add_categories(\"None\")\n",
    "    # Ordinal categories\n",
    "    for name, levels in ordered_levels.items():\n",
    "        df[name] = df[name].astype(CategoricalDtype(levels,\n",
    "                                                    ordered=True))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def impute_upgraded(df):\n",
    "    for name in df.select_dtypes(\"number\").columns:\n",
    "        df[name] = df[name].fillna(df[name].median())\n",
    "    for name in df.select_dtypes(\"category\").columns:\n",
    "        df[name] = df[name].fillna(df[name].mode().iloc[0])\n",
    "    return df\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    data_dir = 'input/'\n",
    "    # data_dir = Path(\"/kaggle/input/playground-series-s5e4/\")\n",
    "\n",
    "\n",
    "    df_train = pd.read_csv(data_dir + 'train.csv', index_col=\"id\")\n",
    "    df_test = pd.read_csv(data_dir + 'test.csv', index_col=\"id\")\n",
    "\n",
    "    #Merge the splits so we can preprocess them together\n",
    "    df = pd.concat([df_train, df_test])\n",
    "    \n",
    "    #Preprocessing\n",
    "    df = clean(df)\n",
    "    df = encode(df)\n",
    "    df = impute_upgraded(df)\n",
    "\n",
    "\n",
    "    #reform splits\n",
    "    df_train = df.loc[df_train.index, :]\n",
    "    df_test = df.loc[df_test.index, :]\n",
    "\n",
    "\n",
    "    return df_train, df_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(df):\n",
    "    X = df.copy()\n",
    "    for colname in X.select_dtypes([\"category\"]):\n",
    "        X[colname] = X[colname].cat.codes\n",
    "    return X\n",
    "\n",
    "cluster_features = [\n",
    "    \"Episode_Length_minutes\",\n",
    "    \"Host_Popularity_percentage\",\n",
    "    \"Guest_Popularity_percentage\",\n",
    "    \"Episode_Num\"\n",
    "]\n",
    "\n",
    "def cluster_labels(df, features, n_clusters=20):\n",
    "    X = df.copy()\n",
    "    X_scaled = X.loc[:, features]\n",
    "    X_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n",
    "    X_new = pd.DataFrame()\n",
    "    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "    return X_new\n",
    "\n",
    "def pca_inspired(df):\n",
    "    X = pd.DataFrame()\n",
    "    X[\"TopFeaturesCombined\"] = df.Episode_Length_minutes * df.Host_Popularity_percentage * df.Episode_Num\n",
    "    X[\"GuestImpact\"] = df.Guest_Popularity_percentage * df.Episode_Length_minutes\n",
    "    X[\"HostImpact\"] = df.Host_Popularity_percentage * df.Episode_Length_minutes\n",
    "    X[\"ContentDensity\"] = df.Episode_Length_minutes / (df.Number_of_Ads + 1)\n",
    "    return X\n",
    "\n",
    "def create_features_categorical(df):\n",
    "    df['is_weekend'] = df['Publication_Day'].apply(lambda x: 1 if x in ['Saturday', 'Sunday'] else 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "    \n",
    "def elm(df_train, df_test=None):\n",
    "    df = df_train.copy()\n",
    "    ELM = {}\n",
    "    for k in range(3):\n",
    "        col_name = f'ELm_r{k}'\n",
    "        df[col_name] = df['Episode_Length_minutes'].round(k)\n",
    "        ELM[col_name] = df[col_name]\n",
    "    return pd.DataFrame(ELM, index=df.index)\n",
    "\n",
    "# reference : https://www.kaggle.com/code/masayakawamata/single-xgboost-add-selected-features/notebook\n",
    "selected_comb = [\n",
    "    # 2-interaction\n",
    "    ['Episode_Length_minutes', 'Host_Popularity_percentage'],\n",
    "    ['Episode_Length_minutes', 'Guest_Popularity_percentage'],\n",
    "    ['Episode_Length_minutes', 'Number_of_Ads'],\n",
    "    ['Episode_Num', 'Host_Popularity_percentage'],\n",
    "    ['Episode_Num', 'Guest_Popularity_percentage'],\n",
    "    ['Episode_Num', 'Number_of_Ads'],    \n",
    "    ['Host_Popularity_percentage', 'Guest_Popularity_percentage'],\n",
    "    ['Host_Popularity_percentage', 'Number_of_Ads'],\n",
    "    ['Host_Popularity_percentage', 'Episode_Sentiment'],\n",
    "    ['Episode_Length_minutes', 'Podcast_Name'],\n",
    "    ['Episode_Num', 'Podcast_Name'],  \n",
    "    ['Guest_Popularity_percentage', 'Podcast_Name'],\n",
    "    ['ELm_r1', 'Episode_Num'],\n",
    "    ['ELm_r1', 'Host_Popularity_percentage'], \n",
    "    ['ELm_r1', 'Guest_Popularity_percentage'],\n",
    "    ['ELm_r2', 'Episode_Num'],\n",
    "    ['ELm_r2', 'Episode_Sentiment'],\n",
    "    ['ELm_r2', 'Publication_Day'],\n",
    "\n",
    "\n",
    "    # 3-interaction\n",
    "    ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage'],\n",
    "    ['Episode_Length_minutes', 'Episode_Num', 'Guest_Popularity_percentage'],\n",
    "    ['Episode_Length_minutes', 'Episode_Num', 'Number_of_Ads'],\n",
    "    ['Episode_Length_minutes', 'Episode_Num', 'Episode_Sentiment'],\n",
    "    ['Episode_Length_minutes', 'Episode_Num', 'Publication_Day'],\n",
    "    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage'],\n",
    "    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Number_of_Ads'],\n",
    "    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Episode_Sentiment'],\n",
    "    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Publication_Day'],\n",
    "    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Publication_Time'],\n",
    "    ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Number_of_Ads'],\n",
    "    ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Publication_Day'],\n",
    "    ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Publication_Time'],\n",
    "    ['Episode_Length_minutes', 'Number_of_Ads', 'Episode_Sentiment'],\n",
    "    ['Episode_Length_minutes', 'Number_of_Ads', 'Publication_Day'],\n",
    "    ['Episode_Length_minutes', 'Episode_Sentiment', 'Publication_Time'],\n",
    "    ['Episode_Num', 'Host_Popularity_percentage', 'Guest_Popularity_percentage'],\n",
    "    ['Episode_Num', 'Host_Popularity_percentage', 'Number_of_Ads'],\n",
    "    ['Episode_Num', 'Host_Popularity_percentage', 'Episode_Sentiment'],\n",
    "    ['Episode_Num', 'Host_Popularity_percentage', 'Publication_Day'],\n",
    "    ['Episode_Num', 'Host_Popularity_percentage', 'Publication_Time'],\n",
    "    ['Episode_Num', 'Host_Popularity_percentage', 'Genre'],\n",
    "    ['Episode_Num', 'Guest_Popularity_percentage', 'Number_of_Ads'],\n",
    "    ['Episode_Num', 'Guest_Popularity_percentage', 'Episode_Sentiment'],\n",
    "    ['Episode_Num', 'Guest_Popularity_percentage', 'Publication_Day'],\n",
    "    ['Episode_Num', 'Guest_Popularity_percentage', 'Publication_Time'],\n",
    "    ['Episode_Num', 'Guest_Popularity_percentage', 'Genre'],\n",
    "    ['Episode_Num', 'Number_of_Ads', 'Episode_Sentiment'],\n",
    "    ['Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads'],\n",
    "    ['Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Episode_Sentiment'],\n",
    "    ['Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Publication_Day'],\n",
    "    ['Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Publication_Time'],\n",
    "    ['Host_Popularity_percentage', 'Number_of_Ads', 'Publication_Day'],\n",
    "\n",
    "    ['Guest_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment'],\n",
    "    ['Guest_Popularity_percentage', 'Number_of_Ads', 'Genre'],   \n",
    "    ['ELm_r1', 'Number_of_Ads', 'Episode_Sentiment'],\n",
    "    ['ELm_r2', 'Number_of_Ads', 'Podcast_Name'],\n",
    "\n",
    "    # 4-interaction\n",
    "    ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Guest_Popularity_percentage'],\n",
    "    ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Number_of_Ads'],\n",
    "    ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Episode_Sentiment'],\n",
    "    ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Publication_Day'],\n",
    "    ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Publication_Time'],\n",
    "    ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Genre'],\n",
    "    ['Episode_Length_minutes', 'Episode_Num', 'Guest_Popularity_percentage', 'Number_of_Ads'],\n",
    "    ['Episode_Length_minutes', 'Episode_Num', 'Guest_Popularity_percentage', 'Episode_Sentiment'],\n",
    "    ['Episode_Length_minutes', 'Episode_Num', 'Guest_Popularity_percentage', 'Publication_Day'],\n",
    "    ['Episode_Length_minutes', 'Episode_Num', 'Guest_Popularity_percentage', 'Publication_Time'],\n",
    "    ['Episode_Length_minutes', 'Episode_Num', 'Number_of_Ads', 'Episode_Sentiment'],\n",
    "    ['Episode_Length_minutes', 'Episode_Num', 'Number_of_Ads', 'Publication_Day'],\n",
    "    ['Episode_Length_minutes', 'Episode_Num', 'Number_of_Ads', 'Publication_Time'],\n",
    "    ['Episode_Length_minutes', 'Episode_Num', 'Publication_Day', 'Publication_Time'],\n",
    "    ['Episode_Length_minutes', 'Episode_Num', 'Publication_Day', 'Genre'],    \n",
    "    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads'],\n",
    "    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Episode_Sentiment'],\n",
    "    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Publication_Day'],\n",
    "    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Publication_Time'],\n",
    "    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment'],\n",
    "    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Number_of_Ads', 'Publication_Day'],\n",
    "    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Publication_Day', 'Publication_Time'],\n",
    "    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Publication_Day', 'Genre'],\n",
    "    ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment'],\n",
    "    ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Publication_Day'],\n",
    "    ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Publication_Time'],\n",
    "    ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Genre'],\n",
    "    ['Episode_Length_minutes', 'Episode_Num', 'Publication_Time', 'Podcast_Name'],\n",
    "\n",
    "    ['Episode_Num', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads'],\n",
    "    ['Episode_Num', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Episode_Sentiment'],\n",
    "    ['Episode_Num', 'Host_Popularity_percentage', 'Number_of_Ads', 'Publication_Day'],\n",
    "    ['Episode_Num', 'Host_Popularity_percentage', 'Number_of_Ads', 'Publication_Time'],\n",
    "    ['Episode_Num', 'Host_Popularity_percentage', 'Episode_Sentiment', 'Publication_Day'],\n",
    "    ['Episode_Num', 'Host_Popularity_percentage', 'Episode_Sentiment', 'Publication_Time'],\n",
    "    ['Episode_Num', 'Host_Popularity_percentage', 'Episode_Sentiment', 'Genre'],\n",
    "    ['Episode_Num', 'Host_Popularity_percentage', 'Publication_Day', 'Publication_Time'],\n",
    "    ['Episode_Num', 'Host_Popularity_percentage', 'Publication_Time', 'Genre'],\n",
    "    ['Episode_Num', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment'],\n",
    "    ['Episode_Num', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Genre'],\n",
    "    ['Episode_Num', 'Host_Popularity_percentage', 'Episode_Sentiment', 'Podcast_Name'],\n",
    "    ['Host_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment', 'Podcast_Name'],\n",
    "    ['Host_Popularity_percentage', 'Number_of_Ads', 'Publication_Day', 'Podcast_Name'],\n",
    "    ['Host_Popularity_percentage', 'Number_of_Ads', 'Publication_Time', 'Podcast_Name'],\n",
    "]\n",
    "\n",
    "def combination_features(df, df_test=None):\n",
    "    X = df.copy()\n",
    "    encoded_columns = []\n",
    "\n",
    "    # Process combinations\n",
    "    for comb in selected_comb:\n",
    "        name = '_'.join(comb)\n",
    "        \n",
    "        if len(comb) == 2:\n",
    "            X[name] = X[comb[0]].astype(str) + '_' + X[comb[1]].astype(str)\n",
    "        elif len(comb) == 3:\n",
    "            X[name] = (X[comb[0]].astype(str) + '_' +\n",
    "                      X[comb[1]].astype(str) + '_' +\n",
    "                      X[comb[2]].astype(str))\n",
    "        elif len(comb) == 4:\n",
    "            X[name] = (X[comb[0]].astype(str) + '_' +\n",
    "                      X[comb[1]].astype(str) + '_' +\n",
    "                      X[comb[2]].astype(str) + '_' +\n",
    "                      X[comb[3]].astype(str))\n",
    "        \n",
    "        encoded_columns.append(name)\n",
    "    \n",
    "    # Convert to categorical\n",
    "    X[encoded_columns] = X[encoded_columns].astype('category')\n",
    "    \n",
    "    # Handle test data if provided\n",
    "    if df_test is not None:\n",
    "        X_test = df_test.copy()\n",
    "        \n",
    "        # Apply same transformations to test data\n",
    "        for comb in selected_comb:\n",
    "            name = '_'.join(comb)\n",
    "            \n",
    "            if len(comb) == 2:\n",
    "                X_test[name] = X_test[comb[0]].astype(str) + '_' + X_test[comb[1]].astype(str)\n",
    "            elif len(comb) == 3:\n",
    "                X_test[name] = (X_test[comb[0]].astype(str) + '_' +\n",
    "                              X_test[comb[1]].astype(str) + '_' +\n",
    "                              X_test[comb[2]].astype(str))\n",
    "            elif len(comb) == 4:\n",
    "                X_test[name] = (X_test[comb[0]].astype(str) + '_' +\n",
    "                              X_test[comb[1]].astype(str) + '_' +\n",
    "                              X_test[comb[2]].astype(str) + '_' +\n",
    "                              X_test[comb[3]].astype(str))\n",
    "        \n",
    "        # Convert test data to categorical\n",
    "        X_test[encoded_columns] = X_test[encoded_columns].astype('category')\n",
    "        \n",
    "        return X[encoded_columns], X_test[encoded_columns]\n",
    "    \n",
    "    return X[encoded_columns]\n",
    "    \n",
    "def get_combination_feature_names(df=None):\n",
    "    encoded_columns = []\n",
    "    for comb in selected_comb:\n",
    "        name = '_'.join(comb)\n",
    "        encoded_columns.append(name)\n",
    "    \n",
    "    return encoded_columns\n",
    "\n",
    "# reference : https://www.kaggle.com/code/masayakawamata/single-xgboost-add-selected-features\n",
    "def target_encode(df_train, df_val, col, target, stats='mean', prefix='TE'):\n",
    "    df_val = df_val.copy()\n",
    "    agg = df_train.groupby(col)[target].agg(stats)    \n",
    "    if isinstance(stats, (list, tuple)):\n",
    "        for s in stats:\n",
    "            colname = f\"{prefix}_{col}_{s}\"\n",
    "            df_val[colname] = df_val[col].map(agg[s]).astype(float)\n",
    "            df_val[colname].fillna(agg[s].mean(), inplace=True)\n",
    "    else:\n",
    "        suffix = stats if isinstance(stats, str) else stats.__name__\n",
    "        colname = f\"{prefix}_{col}_{suffix}\"\n",
    "        df_val[colname] = df_val[col].map(agg).astype(float)\n",
    "        df_val[colname].fillna(agg.mean(), inplace=True)\n",
    "    return df_val\n",
    "\n",
    "# reference: https://www.kaggle.com/code/act18l/say-goodbye-to-ordinalencoder\n",
    "class OrderedTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, cat_cols=None, n_splits=5, smoothing=0):\n",
    "        self.cat_cols   = cat_cols\n",
    "        self.n_splits   = n_splits\n",
    "        self.smoothing  = smoothing       # 0 = no smoothing\n",
    "        self.maps_      = {}              # per‑fold maps\n",
    "        self.global_map = {}              # fit on full data for test set\n",
    "\n",
    "    def _make_fold_map(self, X_col, y):\n",
    "        means = y.groupby(X_col, dropna=False).mean()\n",
    "        if self.smoothing > 0:\n",
    "            counts = y.groupby(X_col, dropna=False).count()\n",
    "            smooth = (counts * means + self.smoothing * y.mean()) / (counts + self.smoothing)\n",
    "            means  = smooth\n",
    "        return {k: r for r, k in enumerate(means.sort_values().index)}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True)\n",
    "        if self.cat_cols is None:\n",
    "            self.cat_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "        kf = KFold(self.n_splits, shuffle=True, random_state=42)\n",
    "        self.maps_ = {col: [None]*self.n_splits for col in self.cat_cols}\n",
    "\n",
    "        for fold, (tr_idx, _) in enumerate(kf.split(X)):\n",
    "            X_tr, y_tr = X.loc[tr_idx], y.loc[tr_idx]\n",
    "            for col in self.cat_cols:\n",
    "                self.maps_[col][fold] = self._make_fold_map(X_tr[col], y_tr)\n",
    "\n",
    "        for col in self.cat_cols:\n",
    "            self.global_map[col] = self._make_fold_map(X[col], y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, fold=None):\n",
    "        \"\"\"\n",
    "        • During CV pass fold index to use fold‑specific maps (leak‑free).\n",
    "        • At inference time (fold=None) uses global map.\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        tgt_maps = {col: (self.global_map[col] if fold is None else self.maps_[col][fold])\n",
    "                    for col in self.cat_cols}\n",
    "        for col, mapping in tgt_maps.items():\n",
    "            X[col] = X[col].map(mapping).fillna(-1).astype(int)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, df_test=None):\n",
    "    X = df.copy()\n",
    "    y = X.pop(\"Listening_Time_minutes\")\n",
    "\n",
    "    if df_test is not None:\n",
    "        X_test = df_test.copy()\n",
    "        X_test.pop(\"Listening_Time_minutes\")\n",
    "        X = pd.concat([X, X_test])\n",
    "\n",
    "    #feature engineering\n",
    "    X = create_features_categorical(X)\n",
    "    X = X.join(cluster_labels(X, cluster_features, n_clusters=20))\n",
    "    X = X.join(pca_inspired(X))\n",
    "    X = label_encode(X)\n",
    "    X = X.join(elm(X))\n",
    "    X = X.join(combination_features(X))\n",
    "    \n",
    "    # Reform splits\n",
    "    if df_test is not None:\n",
    "        X_test = X.loc[df_test.index, :]\n",
    "        X.drop(df_test.index, inplace=True)\n",
    "\n",
    "    \n",
    "    if df_test is not None:\n",
    "        return X, X_test\n",
    "    else:\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Evaluasi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def score_stacking_Kfold(X, y, X_test, base_models, meta_model):\n",
    "\n",
    "    X = X.copy()\n",
    "    X_test = X_test.copy()\n",
    "\n",
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    test_preds = np.zeros(len(X_test))\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "        print(f\"Training fold {fold + 1}/{n_splits}...\")\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # 1. Buat meta-features untuk training dan validasi\n",
    "        X_train_meta = np.zeros((len(X_train), len(base_models)))\n",
    "        X_val_meta = np.zeros((len(X_val), len(base_models)))\n",
    "        X_test_meta = np.zeros((len(X_test), len(base_models)))\n",
    "\n",
    "        for i, model in enumerate(base_models):\n",
    "            model.fit(X_train, y_train)\n",
    "            X_train_meta[:, i] = model.predict(X_train)\n",
    "            X_val_meta[:, i] = model.predict(X_val)\n",
    "            X_test_meta[:, i] += model.predict(X_test) / n_splits  # Rata-rata nanti\n",
    "\n",
    "        # 2. Train meta-model\n",
    "        meta_model.fit(X_train_meta, y_train)\n",
    "\n",
    "        # 3. Predict dan hitung skor di validation\n",
    "        val_pred = meta_model.predict(X_val_meta)\n",
    "        score = rmse(y_val, val_pred)\n",
    "        scores.append(score)\n",
    "\n",
    "        # 4. Predict untuk test set\n",
    "        test_preds += meta_model.predict(X_test_meta) / n_splits\n",
    "\n",
    "        print(f\"Fold {fold + 1} RMSE: {score:.4f}\")\n",
    "\n",
    "    print(f'Optimized Cross-validated RMSE score: {np.mean(scores):.5f} +/- {np.std(scores):.5f}')\n",
    "    print(f'Max RMSE score: {np.max(scores):.5f}')\n",
    "    print(f'Min RMSE score: {np.min(scores):.5f}')\n",
    "\n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = load_data()\n",
    "X_train, X_test = create_features(df_train, df_test)\n",
    "y_train = df_train.loc[:,'Listening_Time_minutes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparation\n",
    "\n",
    "train = X_train.copy()\n",
    "train = train.join(y_train)\n",
    "test = X_test.copy()\n",
    "TARGET = 'Listening_Time_minutes'\n",
    "CATS = ['Podcast_Name', 'Episode_Num', 'Genre', 'Publication_Day', 'Publication_Time', 'Episode_Sentiment']\n",
    "NUMS = ['Episode_Length_minutes', 'Host_Popularity_percentage', \n",
    "        'Guest_Popularity_percentage', 'Number_of_Ads']\n",
    "encoded_columns = get_combination_feature_names()\n",
    "encode_stats = ['mean']\n",
    "FEATURES = NUMS + CATS + encoded_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = [\n",
    "    lambda: XGBRegressor(\n",
    "        tree_method='hist',\n",
    "        max_depth=14,\n",
    "        colsample_bytree=0.5,\n",
    "        subsample=0.9,\n",
    "        n_estimators=50_000,\n",
    "        learning_rate=0.02,\n",
    "        enable_categorical=True,\n",
    "        min_child_weight=10,\n",
    "        early_stopping_rounds=150,\n",
    "    ),\n",
    "    lambda: LGBMRegressor(\n",
    "        random_state = 0,\n",
    "        n_iter=1000,\n",
    "        max_depth=-1,\n",
    "        num_leaves=1024,\n",
    "        colsample_bytree=0.7,\n",
    "        learning_rate=0.03,\n",
    "        objective='l2',\n",
    "        verbosity=-1,\n",
    "        max_bin=1024,\n",
    "    ),\n",
    "    lambda: RandomForestRegressor(n_estimators=500, random_state=42)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS          = 3\n",
    "oof_preds      = np.zeros((len(train), 3))\n",
    "test_preds     = np.zeros((len(test), 3))\n",
    "n_models       = 3\n",
    "\n",
    "\n",
    "for model_idx, base_model in enumerate(base_models):\n",
    "    print(f\"Training base model {model_idx+1}/{len(base_models)}\")\n",
    "    \n",
    "    oof = np.zeros(len(train))\n",
    "    pred = np.zeros(len(test))\n",
    "    \n",
    "    outer_kf = KFold(n_splits=FOLDS, shuffle=True, random_state=0)\n",
    "\n",
    "    for fold, (tr_idx, vl_idx) in enumerate(outer_kf.split(train), 1):\n",
    "        print(f\"--- Model {model_idx+1}, Fold {fold} / {FOLDS} ---\")\n",
    "\n",
    "        X_tr_raw = train.loc[tr_idx, FEATURES].reset_index(drop=True)\n",
    "        y_tr     = train.loc[tr_idx, TARGET].reset_index(drop=True)\n",
    "\n",
    "        X_vl_raw = train.loc[vl_idx, FEATURES].reset_index(drop=True)\n",
    "        y_vl     = train.loc[vl_idx, TARGET].reset_index(drop=True)\n",
    "\n",
    "        X_ts_raw = test[FEATURES].copy()\n",
    "\n",
    "        X_tr, X_vl, X_ts = X_tr_raw.copy(), X_vl_raw.copy(), X_ts_raw.copy()\n",
    "\n",
    "        inner_kf = KFold(n_splits=FOLDS, shuffle=True, random_state=0)\n",
    "        for _, (in_tr_idx, in_vl_idx) in enumerate(inner_kf.split(X_tr_raw), 1):\n",
    "            in_tr = pd.concat([X_tr_raw.loc[in_tr_idx], y_tr.loc[in_tr_idx]], axis=1)\n",
    "            in_vl = X_tr_raw.loc[in_vl_idx].reset_index(drop=True)\n",
    "\n",
    "            for col in encoded_columns:\n",
    "                for stat in encode_stats:\n",
    "                    te_tmp = target_encode(\n",
    "                        in_tr, in_vl.copy(),\n",
    "                        col, TARGET,\n",
    "                        stats=stat, prefix=\"TE\"\n",
    "                    )\n",
    "                    te_col = f\"TE_{col}_{stat}\"\n",
    "                    X_tr.loc[in_vl_idx, te_col] = te_tmp[te_col].values\n",
    "\n",
    "        tr_with_y = pd.concat([X_tr_raw, y_tr], axis=1)\n",
    "        for col in encoded_columns:\n",
    "            for stat in encode_stats:\n",
    "                te_col = f\"TE_{col}_{stat}\"\n",
    "                X_vl = target_encode(tr_with_y, X_vl,      col, TARGET,\n",
    "                                    stats=stat, prefix=\"TE\")\n",
    "                X_ts = target_encode(tr_with_y, X_ts,      col, TARGET,\n",
    "                                    stats=stat, prefix=\"TE\")\n",
    "\n",
    "        X_tr.drop(encoded_columns, axis=1, inplace=True)\n",
    "        X_vl.drop(encoded_columns, axis=1, inplace=True)\n",
    "        X_ts.drop(encoded_columns, axis=1, inplace=True)    \n",
    "\n",
    "        enc = OrderedTargetEncoder(\n",
    "            cat_cols=CATS,\n",
    "            n_splits=FOLDS,\n",
    "            smoothing=20\n",
    "        ).fit(X_tr, y_tr)\n",
    "\n",
    "        X_tr[CATS] = enc.transform(X_tr[CATS], fold=None)[CATS]\n",
    "        X_vl[CATS] = enc.transform(X_vl[CATS], fold=None)[CATS]\n",
    "        X_ts[CATS] = enc.transform(X_ts[CATS], fold=None)[CATS]\n",
    "        \n",
    "        model = base_model()\n",
    "\n",
    "\n",
    "        if isinstance(model, XGBRegressor):\n",
    "            model.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=[(X_vl, y_vl)],\n",
    "                verbose=500\n",
    "            )\n",
    "        else:\n",
    "            model.fit(X_tr, y_tr)\n",
    "\n",
    "        oof[vl_idx]  = model.predict(X_vl)\n",
    "        pred        += model.predict(X_ts) / FOLDS\n",
    "\n",
    "        del X_tr_raw, X_vl_raw, X_ts_raw, X_tr, X_vl, X_ts, y_tr, y_vl\n",
    "        if fold != FOLDS:\n",
    "            del model\n",
    "        gc.collect()\n",
    "\n",
    "    oof_preds[:, model_idx] = oof\n",
    "    test_preds[:, model_idx] = pred\n",
    "    \n",
    "    rmse = root_mean_squared_error(train[TARGET], oof)\n",
    "    print(f\"Base Model {model_idx+1} OOF RMSE: {rmse:.5f}\")\n",
    "\n",
    "\n",
    "print(\"Training meta-model...\")\n",
    "\n",
    "# Konversi hasil OOF dan test predictions ke DataFrame\n",
    "X_meta_train = pd.DataFrame(oof_preds, columns=[f'model_{i+1}' for i in range(len(base_models))])\n",
    "X_meta_test = pd.DataFrame(test_preds, columns=[f'model_{i+1}' for i in range(len(base_models))])\n",
    "\n",
    "ffinal_preds = score_stacking_Kfold(\n",
    "    X=X_meta_train, \n",
    "    y=train[TARGET], \n",
    "    X_test=X_meta_test,\n",
    "    base_models=[\n",
    "        LinearRegression(),\n",
    "        Ridge(alpha=0.5),\n",
    "        ElasticNet(alpha=0.001, l1_ratio=0.5)\n",
    "    ],\n",
    "    meta_model=Ridge(alpha=1.0)\n",
    ")\n",
    "\n",
    "print(\"Stacking complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking Model\n",
    "\n",
    "base_models = [\n",
    "    xgb_model,\n",
    "    random_forest_model,\n",
    "    lgbm_model\n",
    "]\n",
    "\n",
    "meta_model = Ridge(random_state=0)\n",
    "\n",
    "# y_pred_3 = stacking_regression_with_rmse(\n",
    "#     base_models, \n",
    "#     meta_model, \n",
    "#     X_train, \n",
    "#     y_train, \n",
    "#     X_test,\n",
    "#     n_splits=5\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Stacking Model\n",
    "\n",
    "# final_test_prediction = score_stacking_Kfold(\n",
    "#     X_train, \n",
    "#     y_train, \n",
    "#     X_test, \n",
    "#     base_models, \n",
    "#     meta_model\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submisson():\n",
    "    output = pd.DataFrame({'id': X_test.index, 'Listening_Time_minutes': pred})\n",
    "    output.to_csv('my_submission8.csv', index=False)\n",
    "    print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_submisson()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
