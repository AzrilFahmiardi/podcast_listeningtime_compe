{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from category_encoders import MEstimateEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from pathlib import Path\n",
    "from scipy.stats import skew \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './input/'\n",
    "# data_dir = Path(\"../input/house-prices-advanced-regression-techniques/\")\n",
    "X_train = pd.read_csv(data_dir + 'train.csv', index_col=\"id\")\n",
    "X_test = pd.read_csv(data_dir + 'test.csv', index_col=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## EDA (Exploratory Data Analysis)\n",
    "\n",
    "hasil :\n",
    "### missing values :\n",
    "Missing values:\n",
    "- Guest_Popularity_percentage    (146030) (19%)\n",
    "- Episode_Length_minutes          (87093) (12%)\n",
    "- Number_of_Ads                       (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_data(df):\n",
    "    print(f\"DataFrame shape: {df.shape}\")\n",
    "    print(f\"DataFrame columns: {df.columns.tolist()}\")\n",
    "    print(f\"DataFrame info:\\n{df.info()}\")\n",
    "    print(f\"DataFrame description:\\n{df.describe(include='all')}\")\n",
    "    print(f\"Missing values:\\n{df.isnull().sum().sort_values(ascending=False).head(20)}\")\n",
    "    print(f\"Duplicate rows: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "beberapa funtion untuk preprocessing :\n",
    "\n",
    "- clean() - potensi regex\n",
    "- encode() \n",
    "- impute() - masih bisa dikembangin\n",
    "- outlier_check()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disini bisa buat fitur episode_number\n",
    "def clean(df):\n",
    "\n",
    "    df['Episode_Number'] = df['Episode_Title'].str.extract(r'(\\d+)').astype(float)\n",
    "    df = df.drop('Episode_Title', axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df):\n",
    "    # The nominative (unordered) categorical features\n",
    "    features_nom = [\n",
    "    'Podcast_Name',\n",
    "    'Genre',\n",
    "    'Publication_Day',\n",
    "    ]\n",
    "    \n",
    "    features_ord = ['Episode_Sentiment', 'Publication_Time']\n",
    "\n",
    "    ordered_levels = {\n",
    "        'Episode_Sentiment': ['Negative', 'Neutral', 'Positive'],\n",
    "        'Publication_Time': ['Morning', 'Afternoon', 'Evening', 'Night']\n",
    "    }\n",
    "\n",
    "    # Add a None level for missing values\n",
    "    ordered_levels = {key: [\"None\"] + value for key, value in\n",
    "                  ordered_levels.items()}\n",
    "\n",
    "    \n",
    "    # Nominal categories\n",
    "    for name in features_nom:\n",
    "        df[name] = df[name].astype(\"category\")\n",
    "        # Add a None category for missing values\n",
    "        if \"None\" not in df[name].cat.categories:\n",
    "            df[name] = df[name].cat.add_categories(\"None\")\n",
    "    # Ordinal categories\n",
    "    for name, levels in ordered_levels.items():\n",
    "        df[name] = df[name].astype(CategoricalDtype(levels,\n",
    "                                                    ordered=True))\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(df):\n",
    "    for name in df.select_dtypes(\"number\"):\n",
    "        df[name] = df[name].fillna(0)\n",
    "    for name in df.select_dtypes(\"category\"):\n",
    "        df[name] = df[name].fillna(\"None\")\n",
    "    return df\n",
    "\n",
    "def impute_upgraded(df):\n",
    "    for name in df.select_dtypes(\"number\").columns:\n",
    "        df[name] = df[name].fillna(df[name].median())\n",
    "    for name in df.select_dtypes(\"category\").columns:\n",
    "        df[name] = df[name].fillna(df[name].mode().iloc[0])  # mode bisa punya banyak nilai\n",
    "    return df\n",
    "\n",
    "def impute_fillna_mean(df):\n",
    "    # numerical features\n",
    "    for name in df.select_dtypes(\"number\"):\n",
    "        df[name] = df[name].fillna(df[name].mean())\n",
    "    \n",
    "    # categorical features\n",
    "    for name in df.select_dtypes(\"category\"):\n",
    "        df[name] = df[name].fillna(df[name].mode().iloc[0])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def impute_fillna_median(df):\n",
    "    # numerical features\n",
    "    for name in df.select_dtypes(\"number\"):\n",
    "        df[name] = df[name].fillna(df[name].median())\n",
    " \n",
    "    # categorical features\n",
    "    for name in df.select_dtypes(\"category\"):\n",
    "        df[name] = df[name].fillna(df[name].mode().iloc[0])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def impute_simple_mean(df):\n",
    "    #numerical features\n",
    "    num_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    num_imputer = SimpleImputer(strategy='mean')\n",
    "    df[num_features] = num_imputer.fit_transform(df[num_features])\n",
    "\n",
    "    #categorical features\n",
    "    cat_features = df.select_dtypes(include=['object', 'category']).columns\n",
    "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    df[cat_features] = cat_imputer.fit_transform(df[cat_features])\n",
    "\n",
    "    for col in cat_features:\n",
    "        df[col] = df[col].astype('category').cat.codes\n",
    "\n",
    "    return df   \n",
    "\n",
    "def impute_simple_median(df):\n",
    "    #numerical features\n",
    "    num_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df[num_features] = imputer.fit_transform(df[num_features])\n",
    "\n",
    "    #categorical features\n",
    "    cat_features = df.select_dtypes(include=['object', 'category']).columns\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    df[cat_features] = imputer.fit_transform(df[cat_features])\n",
    "\n",
    "    for col in cat_features:\n",
    "        df[col] = df[col].astype('category').cat.codes\n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_check(df, feature, log=False, return_filtered=False, q_low=0.01, q_high=0.99, plot=True):\n",
    "\n",
    "    # Menampilkan grafik distribusi fitur jika plot=True\n",
    "    if plot:\n",
    "        print(f\"ðŸ“Š Distribusi Fitur: {feature}\")\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        sns.histplot(df[feature], bins=100, kde=True, color='skyblue')\n",
    "        plt.title(f'Distribusi asli: {feature}')\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel('Frekuensi')\n",
    "        plt.show()\n",
    "\n",
    "        if log:\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            sns.histplot(np.log1p(df[feature]), bins=100, kde=True, color='salmon')\n",
    "            plt.title(f'Distribusi log(1 + {feature})')\n",
    "            plt.xlabel(f\"log(1 + {feature})\")\n",
    "            plt.ylabel('Frekuensi')\n",
    "            plt.show()\n",
    "\n",
    "    # Menghitung quantile\n",
    "    ql = df[feature].quantile(q_low)\n",
    "    qh = df[feature].quantile(q_high)\n",
    "\n",
    "    if plot:\n",
    "        print(f\"ðŸ” Quantile batas:\")\n",
    "        print(f\" - {int(q_low*100)}th percentile: {ql:.2f}\")\n",
    "        print(f\" - {int(q_high*100)}th percentile: {qh:.2f}\")\n",
    "\n",
    "    # Filter data berdasarkan quantile\n",
    "    df_filtered = df[(df[feature] >= ql) & (df[feature] <= qh)]\n",
    "    if plot:\n",
    "        print(f\"âœ… Jumlah data setelah filter: {df_filtered.shape[0]} dari {df.shape[0]} ({100 * df_filtered.shape[0] / df.shape[0]:.2f}%)\")\n",
    "    \n",
    "    print(\"Sisa data:\", df_filtered.shape)\n",
    "    \n",
    "    # Mengembalikan hasil filter jika return_filtered=True\n",
    "    if return_filtered:\n",
    "        return df_filtered  # Hanya mengembalikan data yang sudah difilter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noOutlier = outlier_check(X_train, 'Episode_Length_minutes', log=True, return_filtered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skewness():\n",
    "    df_train = pd.read_csv(data_dir + 'train.csv', index_col=\"id\")\n",
    "    return df_train\n",
    "\n",
    "numerical_features = [\n",
    "        \"Episode_Length_minutes\",\n",
    "        \"Host_Popularity_percentage\",\n",
    "        \"Guest_Popularity_percentage\",\n",
    "        \"Number_of_Ads\",\n",
    "        \"Listening_Time_minutes\",\n",
    "    ]\n",
    "\n",
    "df_train = skewness()\n",
    "\n",
    "for feature in numerical_features:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    # Histogram with KDE (Kernel Density Estimate)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(df_train[feature], kde=True, bins=30)\n",
    "    plt.title(f\"Histogram of {feature}\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    # Box plot to identify outliers\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x=df_train[feature])\n",
    "    plt.title(f\"Box Plot of {feature}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print additional statistics\n",
    "    print(f\"\\nStatistics for {feature}:\")\n",
    "    print(f\"Skewness: {df_train[feature].skew():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "- load_data()\n",
    "- panggil load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    #Read data\n",
    "    data_dir = 'input/'\n",
    "    # data_dir = Path(\"../input/house-prices-advanced-regression-techniques/\")\n",
    "\n",
    "    df_train = pd.read_csv(data_dir + 'train.csv', index_col=\"id\")\n",
    "    df_test = pd.read_csv(data_dir + 'test.csv', index_col=\"id\")\n",
    "\n",
    "    #Merge the splits so we can preprocess them together\n",
    "    df = pd.concat([df_train, df_test])\n",
    "    #Preprocessing\n",
    "    df = clean(df)\n",
    "    df = encode(df)\n",
    "    df = impute_upgraded(df)\n",
    "\n",
    "    # df['is_weekend'] = df['Publication_Day'].apply(lambda x: 1 if x in ['Saturday', 'Sunday'] else 0)\n",
    "\n",
    "\n",
    "    #reform splits\n",
    "    df_train = df.loc[df_train.index, :]\n",
    "    df_test = df.loc[df_test.index, :]\n",
    "\n",
    "    # df_train = outlier_check(df_train, 'Episode_Length_minutes', log=True, return_filtered=True, plot=False)\n",
    "\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "def load_data_for_baseLine():\n",
    "    data_dir = 'input/'\n",
    "\n",
    "    df_train = pd.read_csv(data_dir + 'train.csv', index_col=\"id\")\n",
    "    df_test = pd.read_csv(data_dir + 'test.csv', index_col=\"id\")\n",
    "\n",
    "    # Simpan panjang data asli untuk df_train dan df_test\n",
    "    train_len = len(df_train)\n",
    "    test_len = len(df_test)\n",
    "\n",
    "    # Gabungkan df_train dan df_test\n",
    "    df = pd.concat([df_train, df_test])\n",
    "\n",
    "    # Preprocessing\n",
    "    df = clean(df)\n",
    "    df = encode(df)\n",
    "    #df = impute_fillna_median(df)\n",
    "    #df = impute_upgraded(df)\n",
    "    df = impute(df)\n",
    "\n",
    "    # df['is_weekend'] = df['Publication_Day'].apply(lambda x: 1 if x in ['Saturday', 'Sunday'] else 0)\n",
    "\n",
    "    \n",
    "    # df['Number_of_Ads_log'] = np.log1p(df['Number_of_Ads'])\n",
    "    # df['Guest_Popularity_percentage_log'] = np.log1p(df['Guest_Popularity_percentage'])\n",
    "    # df['Host_Popularity_percentage_log'] = np.log1p(df['Host_Popularity_percentage'])\n",
    "    # df['Episode_Length_minutes_log'] = np.log1p(df['Episode_Length_minutes']) \n",
    "    \n",
    "    # df = df.drop(columns=['Number_of_Ads'])\n",
    "    # df = df.drop(columns=['Guest_Popularity_percentage'])\n",
    "    # df = df.drop(columns=['Host_Popularity_percentage'])\n",
    "    # df = df.drop(columns=['Episode_Length_minutes'])\n",
    "    \n",
    "\n",
    "    # Pisahkan kembali df_train dan df_test berdasarkan panjang data asli\n",
    "    df_train = df.iloc[:train_len, :]\n",
    "    df_test = df.iloc[train_len:train_len + test_len, :]\n",
    "\n",
    "    # df_train = outlier_check(df_train, 'Episode_Length_minutes', log=True, return_filtered=True, plot=False)\n",
    "\n",
    "\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Base line\n",
    "\n",
    "- score_dataset()\n",
    "- cek liat score\n",
    "\n",
    "Baseline : 13.20727 RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = load_data_for_baseLine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'random_state': 0,\n",
    "    'n_estimators': 565,\n",
    "    'max_depth': 14,\n",
    "    'learning_rate': 0.04222221,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,    \n",
    "    'tree_method':'hist', \n",
    "    # 'tree_method':'gpu_hist', \n",
    "    'n_jobs': -1  \n",
    "}\n",
    "\n",
    "#xgbmodel\n",
    "xgb_model = XGBRegressor(\n",
    "    random_state = 0,\n",
    "    n_estimators = 565,\n",
    "    max_depth= 14,\n",
    "    learning_rate = 0.04222221,\n",
    "    subsample= 0.8,\n",
    "    colsample_bytree = 0.8,   \n",
    "    n_jobs= -1 \n",
    ")\n",
    "\n",
    "#lgbmodel\n",
    "lgbm_model = LGBMRegressor(\n",
    "        random_state = 0,\n",
    "        n_iter=1000,\n",
    "        max_depth=-1,\n",
    "        num_leaves=1024,\n",
    "        colsample_bytree=0.7,\n",
    "        learning_rate=0.03,\n",
    "        objective='l2',\n",
    "        verbosity=-1,\n",
    "        max_bin=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(X, y, model=None):\n",
    "    # Label encoding for categoricals\n",
    "    for colname in X.select_dtypes(\"category\"):\n",
    "        X[colname] = X[colname].cat.codes\n",
    "    \n",
    "    # Cross-validation pakai RMSE\n",
    "    score = cross_val_score(\n",
    "        model, X, y, \n",
    "        cv=5,\n",
    "        scoring=\"neg_mean_squared_error\"\n",
    "    )\n",
    "\n",
    "    score = -1 * score.mean()\n",
    "    score = np.sqrt(score)  #matriknya make rmse\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df_train.copy()\n",
    "y = X.pop(\"Listening_Time_minutes\")\n",
    "\n",
    "baseline_score = score_dataset(X, y, XGBRegressor(random_state=0))\n",
    "print(f\"Baseline score: {baseline_score:.5f} RMSE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline With KFold\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "def score_dataset_Kfold(X,y, X_test):\n",
    "\n",
    "    X = X.copy()\n",
    "    X_test = X_test.copy()\n",
    "    \n",
    "    for colname in X.select_dtypes(\"category\"):\n",
    "        X[colname] = X[colname].cat.codes\n",
    "        \n",
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    test_preds = np.zeros(len(X_test))\n",
    "\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "        print(f\"Training fold {fold + 1}/{n_splits}...\")    \n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]   \n",
    "        model = XGBRegressor()\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=100)    \n",
    "        val_pred = model.predict(X_val)\n",
    "        score = rmse(y_val, val_pred)\n",
    "        scores.append(score)\n",
    "        test_preds += model.predict(X_test) / n_splits      \n",
    "        print(f\"Fold {fold + 1} RMSE: {score:.4f}\")\n",
    "\n",
    "    print(f'Optimized Cross-validated RMSE score: {np.mean(scores):.5f} +/- {np.std(scores):.5f}')\n",
    "    print(f'Max RMSE score: {np.max(scores):.5f}')\n",
    "    print(f'Min RMSE score: {np.min(scores):.5f}')\n",
    "\n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.copy()\n",
    "y = X.pop(\"Listening_Time_minutes\")\n",
    "X_test = df_test.copy()\n",
    "X_test = X_test.drop(columns=[\"Listening_Time_minutes\"], errors='ignore')  # drop kalau ada\n",
    "\n",
    "pred =score_dataset_Kfold(X, y, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Hasil experiment :\n",
    "\n",
    "### Improve\n",
    "- cluster_labels : naik dari 13.20727 jadi 13.20711 RMSE\n",
    "- Feature TopFeaturesCombined : Improvement: 0.179172\n",
    "- Feature GuestImpact : Improvement: 0.149887\n",
    "- Feature HostImpact : Improvement: 0.146457\n",
    "- Feature ContentDensity : Improvement: 0.138797\n",
    "- Impute median : naik dari 13.20727 jadi 13.03053 RMSE \n",
    "- Model Tuning : naik dari 13.20727 jadi 12.82695 RMSE\n",
    "- simpleImputer : naik dari jadi 13.20727 13.03053 RMSE\n",
    "\n",
    "\n",
    "### Degrade\n",
    "- pca_components : turun dari 13.20727 jadi 13.226382669875116\n",
    "- Feature PopularityScore : Improvement: -0.006710\n",
    "- Feature PopularityScore : Improvement: -0.006710\n",
    "- Feature EngagementFactor : Improvement: -0.004424\n",
    "- Feature PodcastMomentum : Improvement: -0.005147\n",
    "- Feature TimeDayInteraction : Improvement: -0.001016\n",
    "- remove outlier malah turun\n",
    "\n",
    "### No Change\n",
    "- NormalizedEpisodeNumber : Improvement: 0.000000\n",
    "- Skew fix: Number_of_Ads, Guest_Popularity_percentage, Host_Popularity_percentage, Episode_Length_minutes : Improvement : 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mi_scores(X, y):\n",
    "    X = X.copy()\n",
    "    for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # All discrete features should now have integer dtypes\n",
    "    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "\n",
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.copy()\n",
    "y = X.pop(\"Listening_Time_minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_scores = make_mi_scores(X, y)\n",
    "print(\"Mutual Information Scores:\")\n",
    "mi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(df):\n",
    "    X = df.copy()\n",
    "    for colname in X.select_dtypes([\"category\"]):\n",
    "        X[colname] = X[colname].cat.codes\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### K-means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-means clustering\n",
    "\n",
    "cluster_features = [\n",
    "    \"Episode_Length_minutes\",\n",
    "    \"Host_Popularity_percentage\",\n",
    "    \"Guest_Popularity_percentage\",\n",
    "    \"Episode_Number\"\n",
    "]\n",
    "\n",
    "\n",
    "def cluster_labels(df, features, n_clusters=20):\n",
    "    X = df.copy()\n",
    "    X_scaled = X.loc[:, features]\n",
    "    X_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)               #TUNING\n",
    "    X_new = pd.DataFrame()\n",
    "    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "    return X_new\n",
    "\n",
    "def cluster_distance(df, features, n_clusters=20):\n",
    "    X = df.copy()\n",
    "    X_scaled = X.loc[:, features]\n",
    "    X_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n",
    "    kmeans = KMeans(n_clusters=20, n_init=50, random_state=0)                          #TUNING\n",
    "    X_cd = kmeans.fit_transform(X_scaled)\n",
    "    # Label features and join to dataset\n",
    "    X_cd = pd.DataFrame(\n",
    "        X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])]\n",
    "    )\n",
    "    return X_cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_labels= cluster_labels(X, cluster_features, n_clusters=20)\n",
    "df_cluster_distance = cluster_distance(X, cluster_features, n_clusters=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = pd.concat([df_cluster_labels, df_cluster_distance], axis=1)\n",
    "df_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scaling ulang agar cocok untuk PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X[cluster_features])\n",
    "\n",
    "# PCA untuk reduksi ke 2 dimensi\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Gabungkan hasil PCA dan label cluster\n",
    "pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
    "pca_df['Cluster'] = df_cluster['Cluster']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(pca_df['PC1'], pca_df['PC2'], c=pca_df['Cluster'], cmap='tab20', alpha=0.7)\n",
    "plt.title('Visualisasi Cluster dengan PCA')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### PCA (Principal Component Analysis)\n",
    "\n",
    "- pca_components()\n",
    "- pca_inspired()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Principal Component Analysis\n",
    "\n",
    "def apply_pca(X, standardize=True):\n",
    "    # Standardize\n",
    "    if standardize:\n",
    "        X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    # Create principal components\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    # Convert to dataframe\n",
    "    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "    X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "    # Create loadings\n",
    "    loadings = pd.DataFrame(\n",
    "        pca.components_.T,  # transpose the matrix of loadings\n",
    "        columns=component_names,  # so the columns are the principal components\n",
    "        index=X.columns,  # and the rows are the original features\n",
    "    )\n",
    "    return pca, X_pca, loadings\n",
    "\n",
    "def plot_variance(pca, width=8, dpi=100):\n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    n = pca.n_components_\n",
    "    grid = np.arange(1, n + 1)\n",
    "    # Explained variance\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    axs[0].bar(grid, evr)\n",
    "    axs[0].set(\n",
    "        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Cumulative Variance\n",
    "    cv = np.cumsum(evr)\n",
    "    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n",
    "    axs[1].set(\n",
    "        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Set up figure\n",
    "    fig.set(figwidth=8, dpi=100)\n",
    "    return axs\n",
    "\n",
    "\n",
    "#gabung-gabungin feature dari analysis PCA\n",
    "def pca_inspired(df):\n",
    "    X = pd.DataFrame()\n",
    "    X[\"TopFeaturesCombined\"] = df.Episode_Length_minutes * df.Host_Popularity_percentage * df.Episode_Number\n",
    "    X[\"GuestImpact\"] = df.Guest_Popularity_percentage * df.Episode_Length_minutes\n",
    "    X[\"HostImpact\"] = df.Host_Popularity_percentage * df.Episode_Length_minutes\n",
    "    X[\"ContentDensity\"] = df.Episode_Length_minutes / (df.Number_of_Ads + 1)\n",
    "    return X\n",
    "\n",
    "def pca_components(df, features):\n",
    "    X = df.loc[:, features]\n",
    "    _, X_pca, _ = apply_pca(X)\n",
    "    return X_pca\n",
    "\n",
    "pca_features = [\n",
    "    \"Episode_Length_minutes\",         # MI tinggi\n",
    "    \"Host_Popularity_percentage\",\n",
    "    \"Guest_Popularity_percentage\",\n",
    "    \"Episode_Number\",\n",
    "    \"Number_of_Ads\"\n",
    "]\n",
    "\n",
    "def create_features_categorical(df):\n",
    "    df['is_weekend'] = df['Publication_Day'].apply(lambda x: 1 if x in ['Saturday', 'Sunday'] else 0)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca_components(X, pca_features)\n",
    "X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = X.loc[:, pca_features]\n",
    "pca, X_pca, loadings = apply_pca(df_pca)\n",
    "plot = plot_variance(pca)\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "#### Mencari Feature terbaik dari untuk pca_inspired()\n",
    "\n",
    "Baseline XGBoost MSRE: 13.207268 Â± 1.076183\n",
    "- With PopularityScore: MSRE = 13.213978 Â± 1.090161 (Improvement: -0.006710)\n",
    "- With HostImpact: MSRE = 13.060811 Â± 0.914555 (Improvement: 0.146457)\n",
    "- With GuestImpact: MSRE = 13.057381 Â± 1.046029 (Improvement: 0.149887)\n",
    "- With NormalizedEpisodeNumber: MSRE = 13.207268 Â± 1.076183 (Improvement: 0.000000)\n",
    "- With EngagementFactor: MSRE = 13.211691 Â± 1.118318 (Improvement: -0.004424)\n",
    "- With PodcastMomentum: MSRE = 13.212414 Â± 1.069311 (Improvement: -0.005147)\n",
    "- With TimeDayInteraction: MSRE = 13.208284 Â± 1.060503 (Improvement: -0.001016)\n",
    "- With ContentDensity: MSRE = 13.068470 Â± 1.030627 (Improvement: 0.138797)\n",
    "- With TopFeaturesCombined: MSRE = 13.028096 Â± 0.956762 (Improvement: 0.179172)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "# Fungsi custom untuk menghitung MSRE (Mean Squared Relative Error)\n",
    "def msre(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean Squared Relative Error: mean((y_true - y_pred)^2 / y_true^2)\n",
    "    \"\"\"\n",
    "    # Hindari pembagian dengan 0\n",
    "    mask = y_true != 0\n",
    "    y_true_safe = y_true[mask]\n",
    "    y_pred_safe = y_pred[mask]\n",
    "    \n",
    "    if len(y_true_safe) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Hitung MSRE\n",
    "    relative_errors = ((y_true_safe - y_pred_safe) ** 2) / (y_true_safe ** 2)\n",
    "    return np.mean(relative_errors)\n",
    "\n",
    "# Buat scorer untuk digunakan dalam cross_val_score\n",
    "msre_scorer = make_scorer(msre, greater_is_better=False)\n",
    "\n",
    "def evaluate_xgb_features(df, target_col, features_list, params=None):\n",
    "\n",
    "    results = {}    \n",
    "    \n",
    "    # Baseline model dengan fitur asli\n",
    "    X_baseline = df.drop(target_col, axis=1)\n",
    "    baseline_features = X_baseline.columns.tolist()\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Evaluasi baseline dengan XGBoost\n",
    "    model = XGBRegressor()\n",
    "    baseline_scores = cross_val_score(model, X_baseline, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    # Negasikan karena make_scorer menghasilkan -MSRE untuk greater_is_better=False\n",
    "    baseline_msre = -np.mean(baseline_scores)\n",
    "    baseline_msre = np.sqrt(baseline_msre)\n",
    "    \n",
    "    results['baseline'] = {\n",
    "        'msre': baseline_msre,\n",
    "        'std': np.std(-baseline_scores),\n",
    "        'features_used': baseline_features\n",
    "    }\n",
    "    \n",
    "    print(f\"Baseline XGBoost MSRE: {baseline_msre:.6f} Â± {np.std(-baseline_scores):.6f}\")\n",
    "    \n",
    "    # Buat dan evaluasi setiap fitur baru secara individual\n",
    "    for feature_name, feature_func in features_list.items():\n",
    "        try:\n",
    "            # Buat fitur baru\n",
    "            new_feature = feature_func(df)\n",
    "            \n",
    "            if isinstance(new_feature, pd.Series):\n",
    "                new_feature = pd.DataFrame({feature_name: new_feature})\n",
    "            \n",
    "            # Gabungkan dengan fitur baseline\n",
    "            X_combined = pd.concat([X_baseline, new_feature], axis=1)\n",
    "            \n",
    "            # Evaluasi dengan XGBoost dan cross-validation\n",
    "            model = XGBRegressor()\n",
    "            scores = cross_val_score(model, X_combined, y, cv=5, scoring='neg_mean_squared_error')\n",
    "            msre_value = -np.mean(scores)  # Negasikan untuk mendapatkan actual MSRE\n",
    "            msre_value = np.sqrt(msre_value) \n",
    "            \n",
    "            # Simpan hasil\n",
    "            results[feature_name] = {\n",
    "                'msre': msre_value,\n",
    "                'std': np.std(-scores),\n",
    "                'improvement': baseline_msre - msre_value,  # Positive means better (lower MSRE)\n",
    "                'features_used': X_combined.columns.tolist()\n",
    "            }\n",
    "            \n",
    "            print(f\"With {feature_name}: MSRE = {msre_value:.6f} Â± {np.std(-scores):.6f} \" +\n",
    "                  f\"(Improvement: {baseline_msre - msre_value:.6f})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {feature_name}: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_xgb_feature_results(results):\n",
    "    \"\"\"\n",
    "    Visualisasi hasil evaluasi fitur\n",
    "    \"\"\"\n",
    "    features = list(results.keys())\n",
    "    msre_values = [results[f]['msre'] for f in features]\n",
    "    improvements = [results[f]['improvement'] if f != 'baseline' else 0 for f in features]\n",
    "    \n",
    "    # Sort by improvement (descending)\n",
    "    sorted_indices = np.argsort([-imp for imp in improvements])\n",
    "    sorted_features = [features[i] for i in sorted_indices]\n",
    "    sorted_improvements = [improvements[i] for i in sorted_indices]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = ['green' if imp > 0 else 'red' for imp in sorted_improvements]\n",
    "    bars = plt.barh(sorted_features, sorted_improvements, color=colors)\n",
    "    \n",
    "    plt.xlabel('MSE Improvement (Positive is better)')\n",
    "    plt.title('Feature Importance by MSE Improvement')\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.5)\n",
    "    \n",
    "    # Tambahkan nilai di setiap bar\n",
    "    for i, bar in enumerate(bars):\n",
    "        value = sorted_improvements[i]\n",
    "        plt.text(value + 0.0001 if value >= 0 else value - 0.005, \n",
    "                 bar.get_y() + bar.get_height()/2, \n",
    "                 f'{value:.6f}', \n",
    "                 va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt\n",
    "\n",
    "\n",
    "# Definisikan fitur yang akan diuji\n",
    "pca_inspired_features = {\n",
    "    \"PopularityScore\": lambda df: df.Host_Popularity_percentage * df.Guest_Popularity_percentage,\n",
    "    \n",
    "    \"HostImpact\": lambda df: df.Host_Popularity_percentage * df.Episode_Length_minutes,\n",
    "    \n",
    "    \"GuestImpact\": lambda df: df.Guest_Popularity_percentage * df.Episode_Length_minutes,\n",
    "    \n",
    "    \"NormalizedEpisodeNumber\": lambda df: df.Episode_Number / df.groupby('Podcast_Name')['Episode_Number'].transform('max'),\n",
    "    \n",
    "    \"EngagementFactor\": lambda df: df.Host_Popularity_percentage + df.Guest_Popularity_percentage + (df.Episode_Sentiment * 100),\n",
    "    \n",
    "    \"PodcastMomentum\": lambda df: df.Episode_Number * df.Host_Popularity_percentage,\n",
    "    \n",
    "    \"TimeDayInteraction\": lambda df: df.Publication_Time + (df.Publication_Day * 24),\n",
    "    \n",
    "    \"ContentDensity\": lambda df: df.Episode_Length_minutes / (df.Number_of_Ads + 1),\n",
    "    \n",
    "    \"TopFeaturesCombined\": lambda df: df.Episode_Length_minutes * df.Host_Popularity_percentage * df.Episode_Number\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train, df_test = load_data_for_baseLine()\n",
    "df_train = label_encode(df_train)\n",
    "results = evaluate_xgb_features(df_train, 'Listening_Time_minutes', pca_inspired_features)\n",
    "plot = plot_xgb_feature_results(results)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrplot(df, method=\"pearson\", annot=True, **kwargs):\n",
    "    sns.clustermap(\n",
    "        df.corr(method, numeric_only=True),\n",
    "        vmin=-1.0,\n",
    "        vmax=1.0,\n",
    "        cmap=\"icefire\",\n",
    "        method=\"complete\",\n",
    "        annot=annot,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "corrplot(df_train, annot=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Train & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, df_test=None):\n",
    "    X = df.copy()\n",
    "    y = X.pop(\"Listening_Time_minutes\")\n",
    "\n",
    "    if df_test is not None:\n",
    "        X_test = df_test.copy()\n",
    "        X_test.pop(\"Listening_Time_minutes\")\n",
    "        X = pd.concat([X, X_test])\n",
    "\n",
    "    X = create_features_categorical(X)\n",
    "    X = X.join(cluster_labels(X, cluster_features, n_clusters=20))\n",
    "    # X = X.join(pca_components(X, pca_features))\n",
    "    X = X.join(pca_inspired(X))\n",
    "\n",
    "    X = label_encode(X)\n",
    "    \n",
    "    # Reform splits\n",
    "    if df_test is not None:\n",
    "        X_test = X.loc[df_test.index, :]\n",
    "        X.drop(df_test.index, inplace=True)\n",
    "\n",
    "    \n",
    "    if df_test is not None:\n",
    "        return X, X_test\n",
    "    else:\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### Evaluasi hasil create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = load_data()\n",
    "X_train, X_test = create_features(df_train, df_test)\n",
    "y_train = df_train.loc[:,'Listening_Time_minutes']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final Score : {score_dataset(X_train, y_train, model=XGBRegressor(**xgb_params)):.5f} RMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def score_dataset_Kfold(X,y, X_test,model=None):\n",
    "\n",
    "    X = X.copy()\n",
    "    X_test = X_test.copy()\n",
    "    \n",
    "    for colname in X.select_dtypes(\"category\"):\n",
    "        X[colname] = X[colname].cat.codes\n",
    "        \n",
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    test_preds = np.zeros(len(X_test))\n",
    "\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "        print(f\"Training fold {fold + 1}/{n_splits}...\")    \n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]   \n",
    "        kmodel = model\n",
    "        kmodel.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=100)    \n",
    "        val_pred = kmodel.predict(X_val)\n",
    "        score = rmse(y_val, val_pred)\n",
    "        scores.append(score)\n",
    "        test_preds += kmodel.predict(X_test) / n_splits      \n",
    "        print(f\"Fold {fold + 1} RMSE: {score:.4f}\")\n",
    "\n",
    "    print(f'Optimized Cross-validated RMSE score: {np.mean(scores):.5f} +/- {np.std(scores):.5f}')\n",
    "    print(f'Max RMSE score: {np.max(scores):.5f}')\n",
    "    print(f'Min RMSE score: {np.min(scores):.5f}')\n",
    "\n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final Score (Random Forest): {score_dataset(X_train, y_train, model=RandomForestRegressor(random_state=0)):.5f} RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without kfold\n",
    "\n",
    "xgb = XGBRegressor(**xgb_params)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with kfold\n",
    "\n",
    "# xgb = XGBRegressor(**xgb_params)\n",
    "# y_pred =score_dataset_Kfold(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "## Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submisson():\n",
    "    output = pd.DataFrame({'id': X_test.index, 'Listening_Time_minutes': y_pred})\n",
    "    output.to_csv('my_submission6.csv', index=False)\n",
    "    print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_submisson()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "## Next Upcoming Experiment\n",
    "\n",
    "\n",
    "1. Manipulasi Text feature \"podcast Name\" dengan regex atau yg lain\n",
    "\n",
    "2. Stacking model\n",
    "\n",
    "\n",
    "3. Interaksi Genre x TimeOfDay â†’ genre tertentu perform lebih bagus pagi/malam?\n",
    "\n",
    "4. Coba k-means clustering untuk Podcast_Name â†’ beri cluster ID sebagai feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Stack Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_stacking_Kfold(X, y, X_test, base_models, meta_model):\n",
    "    \"\"\"\n",
    "    X: training features\n",
    "    y: training targets\n",
    "    X_test: test features\n",
    "    base_models: list of models\n",
    "    meta_model: model for stacking\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    X_test = X_test.copy()\n",
    "\n",
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    test_preds = np.zeros(len(X_test))\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "        print(f\"Training fold {fold + 1}/{n_splits}...\")\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # 1. Buat meta-features untuk training dan validasi\n",
    "        X_train_meta = np.zeros((len(X_train), len(base_models)))\n",
    "        X_val_meta = np.zeros((len(X_val), len(base_models)))\n",
    "        X_test_meta = np.zeros((len(X_test), len(base_models)))\n",
    "\n",
    "        for i, model in enumerate(base_models):\n",
    "            model.fit(X_train, y_train)\n",
    "            X_train_meta[:, i] = model.predict(X_train)\n",
    "            X_val_meta[:, i] = model.predict(X_val)\n",
    "            X_test_meta[:, i] += model.predict(X_test) / n_splits  # Rata-rata nanti\n",
    "\n",
    "        # 2. Train meta-model\n",
    "        meta_model.fit(X_train_meta, y_train)\n",
    "\n",
    "        # 3. Predict dan hitung skor di validation\n",
    "        val_pred = meta_model.predict(X_val_meta)\n",
    "        score = rmse(y_val, val_pred)\n",
    "        scores.append(score)\n",
    "\n",
    "        # 4. Predict untuk test set\n",
    "        test_preds += meta_model.predict(X_test_meta) / n_splits\n",
    "\n",
    "        print(f\"Fold {fold + 1} RMSE: {score:.4f}\")\n",
    "\n",
    "    print(f'Optimized Cross-validated RMSE score: {np.mean(scores):.5f} +/- {np.std(scores):.5f}')\n",
    "    print(f'Max RMSE score: {np.max(scores):.5f}')\n",
    "    print(f'Min RMSE score: {np.min(scores):.5f}')\n",
    "\n",
    "    return test_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking_regression_with_rmse(base_models, meta_model, X_train, y_train, X_test, n_splits=5):\n",
    "    \"\"\"\n",
    "    base_models: list of models\n",
    "    meta_model: stacking model\n",
    "    X_train: training data\n",
    "    y_train: training target\n",
    "    X_test: test data\n",
    "    n_splits: number of splits\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare arrays\n",
    "    n_train, n_test = X_train.shape[0], X_test.shape[0]\n",
    "    n_models = len(base_models)\n",
    "    \n",
    "    base_predictions_train = np.zeros((n_train, n_models))\n",
    "    base_predictions_test = np.zeros((n_test, n_models))\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for i, model in enumerate(base_models):\n",
    "        test_fold_predictions = np.zeros((n_test, n_splits))\n",
    "\n",
    "        for j, (train_idx, valid_idx) in enumerate(kf.split(X_train)):\n",
    "            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n",
    "            y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n",
    "\n",
    "            model.fit(X_tr, y_tr)\n",
    "            base_predictions_train[valid_idx, i] = model.predict(X_val)\n",
    "            test_fold_predictions[:, j] = model.predict(X_test)\n",
    "\n",
    "        base_predictions_test[:, i] = test_fold_predictions.mean(axis=1)\n",
    "\n",
    "    # Train meta model\n",
    "    meta_model.fit(base_predictions_train, y_train)\n",
    "    \n",
    "    # Predict on training set\n",
    "    train_pred = meta_model.predict(base_predictions_train)\n",
    "    final_prediction = meta_model.predict(base_predictions_test)\n",
    "\n",
    "    # Evaluate RMSE\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    for fold, (train_idx, valid_idx) in enumerate(kf.split(base_predictions_train)):\n",
    "        X_val = base_predictions_train[valid_idx]\n",
    "        y_val = y_train.iloc[valid_idx]\n",
    "        val_pred = meta_model.predict(X_val)\n",
    "        score = rmse(y_val, val_pred)\n",
    "        scores.append(score)\n",
    "        print(f\"Fold {fold+1} RMSE: {score:.4f}\")\n",
    "\n",
    "    print(f'Optimized Cross-validated RMSE score: {np.mean(scores):.5f} +/- {np.std(scores):.5f}')\n",
    "    print(f'Max RMSE score: {np.max(scores):.5f}')\n",
    "    print(f'Min RMSE score: {np.min(scores):.5f}')\n",
    "\n",
    "    return final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "base_models = [\n",
    "    xgb,\n",
    "    RandomForestRegressor(random_state=0),\n",
    "    lgbm_model\n",
    "]\n",
    "\n",
    "meta_model = Ridge(random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_prediction = score_stacking_Kfold(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_test, \n",
    "    base_models, \n",
    "    meta_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions_2 = stacking_regression_with_rmse(\n",
    "    base_models, \n",
    "    meta_model, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_test,\n",
    "    n_splits=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_predictions_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submisson():\n",
    "    output = pd.DataFrame({'id': X_test.index, 'Listening_Time_minutes': final_predictions_2})\n",
    "    output.to_csv('my_submission7.csv', index=False)\n",
    "    print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submisson()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
