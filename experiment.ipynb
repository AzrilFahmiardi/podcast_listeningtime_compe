{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from category_encoders import MEstimateEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from pathlib import Path\n",
    "from scipy.stats import skew \n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disini bisa buat fitur episode_number\n",
    "def clean(df):\n",
    "\n",
    "    df['Episode_Number'] = df['Episode_Title'].str.extract(r'(\\d+)').astype(float)\n",
    "    df = df.drop('Episode_Title', axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df):\n",
    "    # The nominative (unordered) categorical features\n",
    "    features_nom = [\n",
    "    'Podcast_Name',\n",
    "    'Genre',\n",
    "    'Publication_Day',\n",
    "    ]\n",
    "    \n",
    "    features_ord = ['Episode_Sentiment', 'Publication_Time']\n",
    "\n",
    "    ordered_levels = {\n",
    "        'Episode_Sentiment': ['Negative', 'Neutral', 'Positive'],\n",
    "        'Publication_Time': ['Morning', 'Afternoon', 'Evening', 'Night']\n",
    "    }\n",
    "\n",
    "    # Add a None level for missing values\n",
    "    ordered_levels = {key: [\"None\"] + value for key, value in\n",
    "                  ordered_levels.items()}\n",
    "\n",
    "    \n",
    "    # Nominal categories\n",
    "    for name in features_nom:\n",
    "        df[name] = df[name].astype(\"category\")\n",
    "        # Add a None category for missing values\n",
    "        if \"None\" not in df[name].cat.categories:\n",
    "            df[name] = df[name].cat.add_categories(\"None\")\n",
    "    # Ordinal categories\n",
    "    for name, levels in ordered_levels.items():\n",
    "        df[name] = df[name].astype(CategoricalDtype(levels,\n",
    "                                                    ordered=True))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(df):\n",
    "    for name in df.select_dtypes(\"number\"):\n",
    "        df[name] = df[name].fillna(0)\n",
    "    for name in df.select_dtypes(\"category\"):\n",
    "        df[name] = df[name].fillna(\"None\")\n",
    "    return df\n",
    "\n",
    "def impute_upgraded(df):\n",
    "    for name in df.select_dtypes(\"number\").columns:\n",
    "        df[name] = df[name].fillna(df[name].median())\n",
    "    for name in df.select_dtypes(\"category\").columns:\n",
    "        df[name] = df[name].fillna(df[name].mode().iloc[0])  # mode bisa punya banyak nilai\n",
    "    return df\n",
    "\n",
    "def impute_fillna_mean(df):\n",
    "    # numerical features\n",
    "    for name in df.select_dtypes(\"number\"):\n",
    "        df[name] = df[name].fillna(df[name].mean())\n",
    "    \n",
    "    # categorical features\n",
    "    for name in df.select_dtypes(\"category\"):\n",
    "        df[name] = df[name].fillna(df[name].mode().iloc[0])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def impute_fillna_median(df):\n",
    "    # numerical features\n",
    "    for name in df.select_dtypes(\"number\"):\n",
    "        df[name] = df[name].fillna(df[name].median())\n",
    " \n",
    "    # categorical features\n",
    "    for name in df.select_dtypes(\"category\"):\n",
    "        df[name] = df[name].fillna(df[name].mode().iloc[0])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def impute_simple_mean(df):\n",
    "    #numerical features\n",
    "    num_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    num_imputer = SimpleImputer(strategy='mean')\n",
    "    df[num_features] = num_imputer.fit_transform(df[num_features])\n",
    "\n",
    "    #categorical features\n",
    "    cat_features = df.select_dtypes(include=['object', 'category']).columns\n",
    "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    df[cat_features] = cat_imputer.fit_transform(df[cat_features])\n",
    "\n",
    "    for col in cat_features:\n",
    "        df[col] = df[col].astype('category').cat.codes\n",
    "\n",
    "    return df   \n",
    "\n",
    "def impute_simple_median(df):\n",
    "    #numerical features\n",
    "    num_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df[num_features] = imputer.fit_transform(df[num_features])\n",
    "\n",
    "    #categorical features\n",
    "    cat_features = df.select_dtypes(include=['object', 'category']).columns\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    df[cat_features] = imputer.fit_transform(df[cat_features])\n",
    "\n",
    "    for col in cat_features:\n",
    "        df[col] = df[col].astype('category').cat.codes\n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'input/'\n",
    "# data_dir = Path(\"/kaggle/input/playground-series-s5e4/\")\n",
    "\n",
    "df_train = pd.read_csv(data_dir + 'train.csv', index_col=\"id\")\n",
    "df_test = pd.read_csv(data_dir + 'test.csv', index_col=\"id\")\n",
    "df_org = pd.read_csv(data_dir + 'org.csv')\n",
    "\n",
    "# Gabungkan train dan original\n",
    "original_clean = df_org.reset_index(drop=True)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "\n",
    "# Gabung train dan original, lalu beri index baru berurutan\n",
    "df_train = pd.concat([df_train, original_clean], ignore_index=True)\n",
    "df_train.index.name = 'id'\n",
    "\n",
    "# Reset dan beri index baru untuk test agar index tidak bentrok\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_test.index = range(len(df_train), len(df_train) + len(df_test))\n",
    "df_test.index.name = 'id'\n",
    "\n",
    "# Gabungkan semua untuk preprocessing\n",
    "df = pd.concat([df_train, df_test])\n",
    "\n",
    "# Preprocessing\n",
    "df = clean(df)\n",
    "df = encode(df)\n",
    "df = impute_upgraded(df)\n",
    "\n",
    "# Bagi kembali menjadi train dan test\n",
    "df_train = df.iloc[:len(df_train)].copy()\n",
    "df_test = df.iloc[len(df_train):].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data_for_baseLine():\n",
    "    # data_dir = 'input/'\n",
    "    data_dir = 'input/'\n",
    "    data_dir = Path(\"/kaggle/input/playground-series-s5e4/\")\n",
    "\n",
    "    df_train = pd.read_csv(data_dir + 'train.csv', index_col=\"id\")\n",
    "    df_test = pd.read_csv(data_dir + 'test.csv', index_col=\"id\")\n",
    "\n",
    "    # Simpan panjang data asli untuk df_train dan df_test\n",
    "    train_len = len(df_train)\n",
    "    test_len = len(df_test)\n",
    "\n",
    "    # Gabungkan df_train dan df_test\n",
    "    df = pd.concat([df_train, df_test])\n",
    "\n",
    "    # Preprocessing\n",
    "    df = clean(df)\n",
    "    df = encode(df)\n",
    "    #df = impute_fillna_median(df)\n",
    "    #df = impute_upgraded(df)\n",
    "    df = impute(df)\n",
    "\n",
    "    # df['is_weekend'] = df['Publication_Day'].apply(lambda x: 1 if x in ['Saturday', 'Sunday'] else 0)\n",
    "\n",
    "    \n",
    "    # df['Number_of_Ads_log'] = np.log1p(df['Number_of_Ads'])\n",
    "    # df['Guest_Popularity_percentage_log'] = np.log1p(df['Guest_Popularity_percentage'])\n",
    "    # df['Host_Popularity_percentage_log'] = np.log1p(df['Host_Popularity_percentage'])\n",
    "    # df['Episode_Length_minutes_log'] = np.log1p(df['Episode_Length_minutes']) \n",
    "    \n",
    "    # df = df.drop(columns=['Number_of_Ads'])\n",
    "    # df = df.drop(columns=['Guest_Popularity_percentage'])\n",
    "    # df = df.drop(columns=['Host_Popularity_percentage'])\n",
    "    # df = df.drop(columns=['Episode_Length_minutes'])\n",
    "    \n",
    "\n",
    "    # Pisahkan kembali df_train dan df_test berdasarkan panjang data asli\n",
    "    df_train = df.iloc[:train_len, :]\n",
    "    df_test = df.iloc[train_len:train_len + test_len, :]\n",
    "\n",
    "    # df_train = outlier_check(df_train, 'Episode_Length_minutes', log=True, return_filtered=True, plot=False)\n",
    "\n",
    "\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = load_data_for_baseLine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.copy()\n",
    "y = X.pop(\"Listening_Time_minutes\")\n",
    "X_test = df_test.copy()\n",
    "X_test = X_test.drop(columns=[\"Listening_Time_minutes\"], errors='ignore')  # drop kalau ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for colname in X.select_dtypes(\"category\"):\n",
    "        X[colname] = X[colname].cat.codes\n",
    "\n",
    "for colname in X_test.select_dtypes(\"category\"):\n",
    "        X_test[colname] = X_test[colname].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline :\n",
    "# XGB : 13.207 RMSE\n",
    "# Random Forest : 12.68481 \n",
    "# LGB : 13.207 RMSE\n",
    "\n",
    "# MRSE : 12.83734 RMSE\n",
    "xgb_params = {\n",
    "    'random_state': 0,\n",
    "    'n_estimators': 565,\n",
    "    'max_depth': 14,\n",
    "    'learning_rate': 0.04222221,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,    \n",
    "    'tree_method':'hist', \n",
    "    # 'tree_method':'gpu_hist', \n",
    "    'n_jobs': -1  \n",
    "}\n",
    "\n",
    "# MRESE : 12.798993843624153.\n",
    "xgb_params_2 = {\n",
    "  'max_depth': 12,\n",
    "  'learning_rate': 0.05858702616823876,\n",
    "  'subsample': 0.9356075676850377,\n",
    "  'colsample_bytree': 0.7895819265828284,\n",
    "  'gamma': 1.7903575391246762\n",
    "}\n",
    "\n",
    "#MRESE : 12.65171 \n",
    "random_forest_params = {\n",
    "    'random_state': 0,\n",
    "    'n_estimators': 500,\n",
    "    'max_depth': 25,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'max_features': 'sqrt',\n",
    "    'bootstrap': True,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#xgbmodel \n",
    "xgb_model = XGBRegressor(\n",
    "    random_state = 0,\n",
    "    n_estimators = 565,\n",
    "    max_depth= 14,\n",
    "    learning_rate = 0.04222221,\n",
    "    subsample= 0.8,\n",
    "    colsample_bytree = 0.8,   \n",
    "    n_jobs= -1 \n",
    ")\n",
    "\n",
    "#lgbmodel\n",
    "lgbm_model = LGBMRegressor(\n",
    "        random_state = 0,\n",
    "        n_iter=1000,\n",
    "        max_depth=-1,\n",
    "        num_leaves=1024,\n",
    "        colsample_bytree=0.7,\n",
    "        learning_rate=0.03,\n",
    "        objective='l2',\n",
    "        verbosity=-1,\n",
    "        max_bin=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(X, y, model=XGBRegressor(random_state=0)):\n",
    "    \n",
    "    # Cross-validation pakai RMSE\n",
    "    score = cross_val_score(\n",
    "        model, X, y, \n",
    "        cv=5,\n",
    "        scoring=\"neg_mean_squared_error\"\n",
    "    )\n",
    "\n",
    "    score = -1 * score.mean()\n",
    "    score = np.sqrt(score)  #matriknya make rmse\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df_train.copy()\n",
    "y = X.pop(\"Listening_Time_minutes\")\n",
    "\n",
    "baseline_score = score_dataset(X, y)\n",
    "print(f\"Baseline score: {baseline_score:.5f} RMSE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline With KFold\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "def score_dataset_Kfold(X,y, X_test):\n",
    "\n",
    "    X = X.copy()\n",
    "    X_test = X_test.copy()\n",
    "    \n",
    "    for colname in X.select_dtypes(\"category\"):\n",
    "        X[colname] = X[colname].cat.codes\n",
    "        \n",
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    test_preds = np.zeros(len(X_test))\n",
    "\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "        print(f\"Training fold {fold + 1}/{n_splits}...\")    \n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]   \n",
    "        model = XGBRegressor()\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=100)    \n",
    "        val_pred = model.predict(X_val)\n",
    "        score = rmse(y_val, val_pred)\n",
    "        scores.append(score)\n",
    "        test_preds += model.predict(X_test) / n_splits      \n",
    "        print(f\"Fold {fold + 1} RMSE: {score:.4f}\")\n",
    "\n",
    "    print(f'Optimized Cross-validated RMSE score: {np.mean(scores):.5f} +/- {np.std(scores):.5f}')\n",
    "    print(f'Max RMSE score: {np.max(scores):.5f}')\n",
    "    print(f'Min RMSE score: {np.min(scores):.5f}')\n",
    "\n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred =score_dataset_Kfold(X, y, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X, label=y)\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'n_estimators': 10000,  # gede, biar dihentikan early\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "    }\n",
    "\n",
    "    # XGBoost CV dengan early stopping\n",
    "    cv_result = xgb.cv(\n",
    "        params=param,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=10000,\n",
    "        nfold=3,\n",
    "        early_stopping_rounds=50,\n",
    "        metrics=\"rmse\",\n",
    "        seed=0,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    # Ambil RMSE minimum (fold terbaik)\n",
    "    best_rmse = cv_result['test-rmse-mean'].min()\n",
    "    return best_rmse\n",
    "\n",
    "# Buat studi Optuna\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Hasil\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "print(\"Best RMSE:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "- baseline score :  12.68481 RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Sample hyperparameters using Optuna\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 500)  # number of trees\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 20)  # max depth of each tree\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)  # minimum samples to split a node\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20)  # minimum samples per leaf\n",
    "\n",
    "    # Create a RandomForestRegressor with the sampled hyperparameters\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Use cross-validation to evaluate the model\n",
    "    score = cross_val_score(model, X, y, cv=3, scoring='neg_root_mean_squared_error')  # We use RMSE here\n",
    "    return -np.mean(score)  # Negative because cross_val_score returns negative values\n",
    "\n",
    "# Create an Optuna study to minimize RMSE\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)  # Perform 50 trials\n",
    "\n",
    "# Print the best hyperparameters and the best RMSE found\n",
    "print(\"Random FOrest Best parameters:\", study.best_params)\n",
    "print(\"Best RMSE:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "baseline_score = score_dataset(X, y, RandomForestRegressor(random_state=0))\n",
    "print(f\"RandomForest Baseline score: {baseline_score:.5f} RMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score = score_dataset(X, y, RandomForestRegressor(**random_forest_params))\n",
    "print(f\"RandomForest params score: {score:.5f} RMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_stacking_Kfold(X, y, X_test, base_models, meta_model):\n",
    "    \"\"\"\n",
    "    X: training features\n",
    "    y: training targets\n",
    "    X_test: test features\n",
    "    base_models: list of models\n",
    "    meta_model: model for stacking\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    X_test = X_test.copy()\n",
    "\n",
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    test_preds = np.zeros(len(X_test))\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "        print(f\"Training fold {fold + 1}/{n_splits}...\")\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # 1. Buat meta-features untuk training dan validasi\n",
    "        X_train_meta = np.zeros((len(X_train), len(base_models)))\n",
    "        X_val_meta = np.zeros((len(X_val), len(base_models)))\n",
    "        X_test_meta = np.zeros((len(X_test), len(base_models)))\n",
    "\n",
    "        for i, model in enumerate(base_models):\n",
    "            model.fit(X_train, y_train)\n",
    "            X_train_meta[:, i] = model.predict(X_train)\n",
    "            X_val_meta[:, i] = model.predict(X_val)\n",
    "            X_test_meta[:, i] += model.predict(X_test) / n_splits  # Rata-rata nanti\n",
    "\n",
    "        # 2. Train meta-model\n",
    "        meta_model.fit(X_train_meta, y_train)\n",
    "\n",
    "        # 3. Predict dan hitung skor di validation\n",
    "        val_pred = meta_model.predict(X_val_meta)\n",
    "        score = rmse(y_val, val_pred)\n",
    "        scores.append(score)\n",
    "\n",
    "        # 4. Predict untuk test set\n",
    "        test_preds += meta_model.predict(X_test_meta) / n_splits\n",
    "\n",
    "        print(f\"Fold {fold + 1} RMSE: {score:.4f}\")\n",
    "\n",
    "    print(f'Optimized Cross-validated RMSE score: {np.mean(scores):.5f} +/- {np.std(scores):.5f}')\n",
    "    print(f'Max RMSE score: {np.max(scores):.5f}')\n",
    "    print(f'Min RMSE score: {np.min(scores):.5f}')\n",
    "\n",
    "    return test_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking_regression_with_rmse(base_models, meta_model, X_train, y_train, X_test, n_splits=5):\n",
    "\n",
    "    # Prepare arrays\n",
    "    n_train, n_test = X_train.shape[0], X_test.shape[0]\n",
    "    n_models = len(base_models)\n",
    "    \n",
    "    base_predictions_train = np.zeros((n_train, n_models))\n",
    "    base_predictions_test = np.zeros((n_test, n_models))\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for i, model in enumerate(base_models):\n",
    "        test_fold_predictions = np.zeros((n_test, n_splits))\n",
    "\n",
    "        for j, (train_idx, valid_idx) in enumerate(kf.split(X_train)):\n",
    "            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n",
    "            y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n",
    "\n",
    "            model.fit(X_tr, y_tr)\n",
    "            base_predictions_train[valid_idx, i] = model.predict(X_val)\n",
    "            test_fold_predictions[:, j] = model.predict(X_test)\n",
    "\n",
    "        base_predictions_test[:, i] = test_fold_predictions.mean(axis=1)\n",
    "\n",
    "    # Train meta model\n",
    "    meta_model.fit(base_predictions_train, y_train)\n",
    "    \n",
    "    # Predict on training set\n",
    "    train_pred = meta_model.predict(base_predictions_train)\n",
    "    final_prediction = meta_model.predict(base_predictions_test)\n",
    "\n",
    "    # Evaluate RMSE\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    for fold, (train_idx, valid_idx) in enumerate(kf.split(base_predictions_train)):\n",
    "        X_val = base_predictions_train[valid_idx]\n",
    "        y_val = y_train.iloc[valid_idx]\n",
    "        val_pred = meta_model.predict(X_val)\n",
    "        score = rmse(y_val, val_pred)\n",
    "        scores.append(score)\n",
    "        print(f\"Fold {fold+1} RMSE: {score:.4f}\")\n",
    "\n",
    "    print(f'Optimized Cross-validated RMSE score: {np.mean(scores):.5f} +/- {np.std(scores):.5f}')\n",
    "    print(f'Max RMSE score: {np.max(scores):.5f}')\n",
    "    print(f'Min RMSE score: {np.min(scores):.5f}')\n",
    "\n",
    "    return final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "base_models = [\n",
    "    xgb_model,\n",
    "    RandomForestRegressor(random_state=0),\n",
    "    lgbm_model\n",
    "]\n",
    "\n",
    "meta_model = Ridge(random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_predictions_2 = stacking_regression_with_rmse(\n",
    "#     base_models, \n",
    "#     meta_model, \n",
    "#     X, \n",
    "#     y, \n",
    "#     X_test,\n",
    "#     n_splits=5\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_test_prediction = score_stacking_Kfold(\n",
    "#     X, \n",
    "#     y, \n",
    "#     X_test, \n",
    "#     base_models, \n",
    "#     meta_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Next Upcoming Experiment\n",
    "\n",
    "\n",
    "1. Manipulasi Text feature \"podcast Name\" dengan regex atau yg lain\n",
    "\n",
    "2. Stacking model\n",
    "\n",
    "\n",
    "3. Interaksi Genre x TimeOfDay → genre tertentu perform lebih bagus pagi/malam?\n",
    "\n",
    "4. Coba k-means clustering untuk Podcast_Name → beri cluster ID sebagai feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
