{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from category_encoders import MEstimateEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from pathlib import Path\n",
    "from scipy.stats import skew \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "\n",
    "    df['Episode_Number'] = df['Episode_Title'].str.extract(r'(\\d+)').astype(float)\n",
    "    df = df.drop('Episode_Title', axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def encode(df):\n",
    "    # The nominative (unordered) categorical features\n",
    "    features_nom = [\n",
    "    'Podcast_Name',\n",
    "    'Genre',\n",
    "    'Publication_Day',\n",
    "    ]\n",
    "    \n",
    "    # The ordinal (ordered) categorical features\n",
    "    ordered_levels = {\n",
    "        'Episode_Sentiment': ['Negative', 'Neutral', 'Positive'],\n",
    "        'Publication_Time': ['Morning', 'Afternoon', 'Evening', 'Night']\n",
    "    }\n",
    "\n",
    "    ordered_levels = {key: [\"None\"] + value for key, value in\n",
    "                  ordered_levels.items()}\n",
    "\n",
    "    \n",
    "    # Nominal categories\n",
    "    for name in features_nom:\n",
    "        df[name] = df[name].astype(\"category\")\n",
    "        if \"None\" not in df[name].cat.categories:\n",
    "            df[name] = df[name].cat.add_categories(\"None\")\n",
    "    # Ordinal categories\n",
    "    for name, levels in ordered_levels.items():\n",
    "        df[name] = df[name].astype(CategoricalDtype(levels,\n",
    "                                                    ordered=True))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def impute_upgraded(df):\n",
    "    for name in df.select_dtypes(\"number\").columns:\n",
    "        df[name] = df[name].fillna(df[name].median())\n",
    "    for name in df.select_dtypes(\"category\").columns:\n",
    "        df[name] = df[name].fillna(df[name].mode().iloc[0])\n",
    "    return df\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    data_dir = 'input/'\n",
    "    # data_dir = Path(\"/kaggle/input/playground-series-s5e4/\")\n",
    "\n",
    "\n",
    "    df_train = pd.read_csv(data_dir + 'train.csv', index_col=\"id\")\n",
    "    df_test = pd.read_csv(data_dir + 'test.csv', index_col=\"id\")\n",
    "\n",
    "    #Merge the splits so we can preprocess them together\n",
    "    df = pd.concat([df_train, df_test])\n",
    "    \n",
    "    #Preprocessing\n",
    "    df = clean(df)\n",
    "    df = encode(df)\n",
    "    df = impute_upgraded(df)\n",
    "\n",
    "    # df['is_weekend'] = df['Publication_Day'].apply(lambda x: 1 if x in ['Saturday', 'Sunday'] else 0)\n",
    "\n",
    "\n",
    "    #reform splits\n",
    "    df_train = df.loc[df_train.index, :]\n",
    "    df_test = df.loc[df_test.index, :]\n",
    "\n",
    "    # df_train = outlier_check(df_train, 'Episode_Length_minutes', log=True, return_filtered=True, plot=False)\n",
    "\n",
    "    return df_train, df_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(df):\n",
    "    X = df.copy()\n",
    "    for colname in X.select_dtypes([\"category\"]):\n",
    "        X[colname] = X[colname].cat.codes\n",
    "    return X\n",
    "\n",
    "cluster_features = [\n",
    "    \"Episode_Length_minutes\",\n",
    "    \"Host_Popularity_percentage\",\n",
    "    \"Guest_Popularity_percentage\",\n",
    "    \"Episode_Number\"\n",
    "]\n",
    "\n",
    "def cluster_labels(df, features, n_clusters=20):\n",
    "    X = df.copy()\n",
    "    X_scaled = X.loc[:, features]\n",
    "    X_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)               #TUNING\n",
    "    X_new = pd.DataFrame()\n",
    "    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "    return X_new\n",
    "\n",
    "def pca_inspired(df):\n",
    "    X = pd.DataFrame()\n",
    "    X[\"TopFeaturesCombined\"] = df.Episode_Length_minutes * df.Host_Popularity_percentage * df.Episode_Number\n",
    "    X[\"GuestImpact\"] = df.Guest_Popularity_percentage * df.Episode_Length_minutes\n",
    "    X[\"HostImpact\"] = df.Host_Popularity_percentage * df.Episode_Length_minutes\n",
    "    X[\"ContentDensity\"] = df.Episode_Length_minutes / (df.Number_of_Ads + 1)\n",
    "    return X\n",
    "\n",
    "def create_features_categorical(df):\n",
    "    df['is_weekend'] = df['Publication_Day'].apply(lambda x: 1 if x in ['Saturday', 'Sunday'] else 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "    \n",
    "def elm(df_train, df_test=None):\n",
    "    df = df_train.copy()\n",
    "    ELM = {}\n",
    "    for k in range(3):\n",
    "        col_name = f'ELm_r{k}'\n",
    "        df[col_name] = df['Episode_Length_minutes'].round(k)\n",
    "        ELM[col_name] = df[col_name]\n",
    "    return pd.DataFrame(ELM, index=df.index)\n",
    "\n",
    "\n",
    "def combination_features(df, df_test=None):\n",
    "    X = df.copy()\n",
    "\n",
    "\n",
    "    train = train.copy()\n",
    "    test = test.copy()\n",
    "    encoded_columns = []\n",
    "\n",
    "    selected_comb = [\n",
    "        # 2-interaction\n",
    "        ['Episode_Length_minutes', 'Host_Popularity_percentage'],\n",
    "        ['Episode_Length_minutes', 'Guest_Popularity_percentage'],\n",
    "        ['Episode_Length_minutes', 'Number_of_Ads'],\n",
    "        ['Episode_Num', 'Host_Popularity_percentage'],\n",
    "        ['Episode_Num', 'Guest_Popularity_percentage'],\n",
    "        ['Episode_Num', 'Number_of_Ads'],    \n",
    "        ['Host_Popularity_percentage', 'Guest_Popularity_percentage'],\n",
    "        ['Host_Popularity_percentage', 'Number_of_Ads'],\n",
    "        ['Host_Popularity_percentage', 'Episode_Sentiment'],\n",
    "        ['Episode_Length_minutes', 'Podcast_Name'],\n",
    "        ['Episode_Num', 'Podcast_Name'],  \n",
    "        ['Guest_Popularity_percentage', 'Podcast_Name'],\n",
    "        ['ELm_r1', 'Episode_Num'],\n",
    "        ['ELm_r1', 'Host_Popularity_percentage'], \n",
    "        ['ELm_r1', 'Guest_Popularity_percentage'],\n",
    "        ['ELm_r2', 'Episode_Num'],\n",
    "        ['ELm_r2', 'Episode_Sentiment'],\n",
    "        ['ELm_r2', 'Publication_Day'],\n",
    "\n",
    "        \n",
    "        # 3-interaction\n",
    "        ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage'],\n",
    "        ['Episode_Length_minutes', 'Episode_Num', 'Guest_Popularity_percentage'],\n",
    "        ['Episode_Length_minutes', 'Episode_Num', 'Number_of_Ads'],\n",
    "        ['Episode_Length_minutes', 'Episode_Num', 'Episode_Sentiment'],\n",
    "        ['Episode_Length_minutes', 'Episode_Num', 'Publication_Day'],\n",
    "        ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage'],\n",
    "        ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Number_of_Ads'],\n",
    "        ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Episode_Sentiment'],\n",
    "        ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Publication_Day'],\n",
    "        ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Publication_Time'],\n",
    "        ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Number_of_Ads'],\n",
    "        ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Publication_Day'],\n",
    "        ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Publication_Time'],\n",
    "        ['Episode_Length_minutes', 'Number_of_Ads', 'Episode_Sentiment'],\n",
    "        ['Episode_Length_minutes', 'Number_of_Ads', 'Publication_Day'],\n",
    "        ['Episode_Length_minutes', 'Episode_Sentiment', 'Publication_Time'],\n",
    "        ['Episode_Num', 'Host_Popularity_percentage', 'Guest_Popularity_percentage'],\n",
    "        ['Episode_Num', 'Host_Popularity_percentage', 'Number_of_Ads'],\n",
    "        ['Episode_Num', 'Host_Popularity_percentage', 'Episode_Sentiment'],\n",
    "        ['Episode_Num', 'Host_Popularity_percentage', 'Publication_Day'],\n",
    "        ['Episode_Num', 'Host_Popularity_percentage', 'Publication_Time'],\n",
    "        ['Episode_Num', 'Host_Popularity_percentage', 'Genre'],\n",
    "        ['Episode_Num', 'Guest_Popularity_percentage', 'Number_of_Ads'],\n",
    "        ['Episode_Num', 'Guest_Popularity_percentage', 'Episode_Sentiment'],\n",
    "        ['Episode_Num', 'Guest_Popularity_percentage', 'Publication_Day'],\n",
    "        ['Episode_Num', 'Guest_Popularity_percentage', 'Publication_Time'],\n",
    "        ['Episode_Num', 'Guest_Popularity_percentage', 'Genre'],\n",
    "        ['Episode_Num', 'Number_of_Ads', 'Episode_Sentiment'],\n",
    "        ['Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads'],\n",
    "        ['Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Episode_Sentiment'],\n",
    "        ['Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Publication_Day'],\n",
    "        ['Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Publication_Time'],\n",
    "        ['Host_Popularity_percentage', 'Number_of_Ads', 'Publication_Day'],\n",
    "\n",
    "        ['Guest_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment'],\n",
    "        ['Guest_Popularity_percentage', 'Number_of_Ads', 'Genre'],   \n",
    "        ['ELm_r1', 'Number_of_Ads', 'Episode_Sentiment'],\n",
    "        ['ELm_r2', 'Number_of_Ads', 'Podcast_Name'],\n",
    "        \n",
    "        # 4-interaction\n",
    "        ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Guest_Popularity_percentage'],\n",
    "        ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Number_of_Ads'],\n",
    "        ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Episode_Sentiment'],\n",
    "        ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Publication_Day'],\n",
    "        ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Publication_Time'],\n",
    "        ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Genre'],\n",
    "        ['Episode_Length_minutes', 'Episode_Num', 'Guest_Popularity_percentage', 'Number_of_Ads'],\n",
    "        ['Episode_Length_minutes', 'Episode_Num', 'Guest_Popularity_percentage', 'Episode_Sentiment'],\n",
    "        ['Episode_Length_minutes', 'Episode_Num', 'Guest_Popularity_percentage', 'Publication_Day'],\n",
    "        ['Episode_Length_minutes', 'Episode_Num', 'Guest_Popularity_percentage', 'Publication_Time'],\n",
    "        ['Episode_Length_minutes', 'Episode_Num', 'Number_of_Ads', 'Episode_Sentiment'],\n",
    "        ['Episode_Length_minutes', 'Episode_Num', 'Number_of_Ads', 'Publication_Day'],\n",
    "        ['Episode_Length_minutes', 'Episode_Num', 'Number_of_Ads', 'Publication_Time'],\n",
    "        ['Episode_Length_minutes', 'Episode_Num', 'Publication_Day', 'Publication_Time'],\n",
    "        ['Episode_Length_minutes', 'Episode_Num', 'Publication_Day', 'Genre'],    \n",
    "        ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads'],\n",
    "        ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Episode_Sentiment'],\n",
    "        ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Publication_Day'],\n",
    "        ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Publication_Time'],\n",
    "        ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment'],\n",
    "        ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Number_of_Ads', 'Publication_Day'],\n",
    "        ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Publication_Day', 'Publication_Time'],\n",
    "        ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Publication_Day', 'Genre'],\n",
    "        ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment'],\n",
    "        ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Publication_Day'],\n",
    "        ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Publication_Time'],\n",
    "        ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Genre'],\n",
    "        ['Episode_Length_minutes', 'Episode_Num', 'Publication_Time', 'Podcast_Name'],\n",
    "        \n",
    "        ['Episode_Num', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads'],\n",
    "        ['Episode_Num', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Episode_Sentiment'],\n",
    "        ['Episode_Num', 'Host_Popularity_percentage', 'Number_of_Ads', 'Publication_Day'],\n",
    "        ['Episode_Num', 'Host_Popularity_percentage', 'Number_of_Ads', 'Publication_Time'],\n",
    "        ['Episode_Num', 'Host_Popularity_percentage', 'Episode_Sentiment', 'Publication_Day'],\n",
    "        ['Episode_Num', 'Host_Popularity_percentage', 'Episode_Sentiment', 'Publication_Time'],\n",
    "        ['Episode_Num', 'Host_Popularity_percentage', 'Episode_Sentiment', 'Genre'],\n",
    "        ['Episode_Num', 'Host_Popularity_percentage', 'Publication_Day', 'Publication_Time'],\n",
    "        ['Episode_Num', 'Host_Popularity_percentage', 'Publication_Time', 'Genre'],\n",
    "        ['Episode_Num', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment'],\n",
    "        ['Episode_Num', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Genre'],\n",
    "        ['Episode_Num', 'Host_Popularity_percentage', 'Episode_Sentiment', 'Podcast_Name'],\n",
    "        ['Host_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment', 'Podcast_Name'],\n",
    "        ['Host_Popularity_percentage', 'Number_of_Ads', 'Publication_Day', 'Podcast_Name'],\n",
    "        ['Host_Popularity_percentage', 'Number_of_Ads', 'Publication_Time', 'Podcast_Name'],\n",
    "        \n",
    "    ]\n",
    "\n",
    "    # Process combinations\n",
    "    for comb in selected_comb:\n",
    "        name = '_'.join(comb)\n",
    "        \n",
    "        if len(comb) == 2:\n",
    "            X[name] = X[comb[0]].astype(str) + '_' + X[comb[1]].astype(str)\n",
    "        elif len(comb) == 3:\n",
    "            X[name] = (X[comb[0]].astype(str) + '_' +\n",
    "                      X[comb[1]].astype(str) + '_' +\n",
    "                      X[comb[2]].astype(str))\n",
    "        elif len(comb) == 4:\n",
    "            X[name] = (X[comb[0]].astype(str) + '_' +\n",
    "                      X[comb[1]].astype(str) + '_' +\n",
    "                      X[comb[2]].astype(str) + '_' +\n",
    "                      X[comb[3]].astype(str))\n",
    "        \n",
    "        encoded_columns.append(name)\n",
    "    \n",
    "    # Convert to categorical\n",
    "    X[encoded_columns] = X[encoded_columns].astype('category')\n",
    "    \n",
    "    # Handle test data if provided\n",
    "    if df_test is not None:\n",
    "        X_test = df_test.copy()\n",
    "        \n",
    "        # Apply same transformations to test data\n",
    "        for comb in selected_comb:\n",
    "            name = '_'.join(comb)\n",
    "            \n",
    "            if len(comb) == 2:\n",
    "                X_test[name] = X_test[comb[0]].astype(str) + '_' + X_test[comb[1]].astype(str)\n",
    "            elif len(comb) == 3:\n",
    "                X_test[name] = (X_test[comb[0]].astype(str) + '_' +\n",
    "                              X_test[comb[1]].astype(str) + '_' +\n",
    "                              X_test[comb[2]].astype(str))\n",
    "            elif len(comb) == 4:\n",
    "                X_test[name] = (X_test[comb[0]].astype(str) + '_' +\n",
    "                              X_test[comb[1]].astype(str) + '_' +\n",
    "                              X_test[comb[2]].astype(str) + '_' +\n",
    "                              X_test[comb[3]].astype(str))\n",
    "        \n",
    "        # Convert test data to categorical\n",
    "        X_test[encoded_columns] = X_test[encoded_columns].astype('category')\n",
    "        \n",
    "        return X[encoded_columns], X_test[encoded_columns]\n",
    "    \n",
    "    return X[encoded_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, df_test=None):\n",
    "    X = df.copy()\n",
    "    y = X.pop(\"Listening_Time_minutes\")\n",
    "\n",
    "    if df_test is not None:\n",
    "        X_test = df_test.copy()\n",
    "        X_test.pop(\"Listening_Time_minutes\")\n",
    "        X = pd.concat([X, X_test])\n",
    "\n",
    "    X = create_features_categorical(X)\n",
    "    # X = X.join(elm(X))\n",
    "    X = X.join(combination_features(X))\n",
    "    X = X.join(cluster_labels(X, cluster_features, n_clusters=20))\n",
    "    # X = X.join(pca_components(X, pca_features))\n",
    "    X = X.join(pca_inspired(X))\n",
    "\n",
    "    X = label_encode(X)\n",
    "    \n",
    "    # Reform splits\n",
    "    if df_test is not None:\n",
    "        X_test = X.loc[df_test.index, :]\n",
    "        X.drop(df_test.index, inplace=True)\n",
    "\n",
    "    \n",
    "    if df_test is not None:\n",
    "        return X, X_test\n",
    "    else:\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Evaluasi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(X, y, model=None):\n",
    "    \n",
    "    score = cross_val_score(\n",
    "        model, X, y, \n",
    "        cv=5,\n",
    "        scoring=\"neg_mean_squared_error\"\n",
    "    )\n",
    "\n",
    "    #RMSE\n",
    "    score = -1 * score.mean()\n",
    "    score = np.sqrt(score) \n",
    "    return score\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def score_dataset_Kfold(X,y, X_test,model=None):\n",
    "\n",
    "    X = X.copy()\n",
    "    X_test = X_test.copy()\n",
    "    \n",
    "    for colname in X.select_dtypes(\"category\"):\n",
    "        X[colname] = X[colname].cat.codes\n",
    "        \n",
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    test_preds = np.zeros(len(X_test))\n",
    "\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "        print(f\"Training fold {fold + 1}/{n_splits}...\")    \n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]   \n",
    "        kmodel = model\n",
    "        kmodel.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=100)    \n",
    "        val_pred = kmodel.predict(X_val)\n",
    "        score = rmse(y_val, val_pred)\n",
    "        scores.append(score)\n",
    "        test_preds += kmodel.predict(X_test) / n_splits      \n",
    "        print(f\"Fold {fold + 1} RMSE: {score:.4f}\")\n",
    "\n",
    "    print(f'Optimized Cross-validated RMSE score: {np.mean(scores):.5f} +/- {np.std(scores):.5f}')\n",
    "    print(f'Max RMSE score: {np.max(scores):.5f}')\n",
    "    print(f'Min RMSE score: {np.min(scores):.5f}')\n",
    "\n",
    "    return test_preds\n",
    "\n",
    "def stacking_regression_with_rmse(base_models, meta_model, X_train, y_train, X_test, n_splits=5):\n",
    "\n",
    "    # Prepare arrays\n",
    "    n_train, n_test = X_train.shape[0], X_test.shape[0]\n",
    "    n_models = len(base_models)\n",
    "    \n",
    "    base_predictions_train = np.zeros((n_train, n_models))\n",
    "    base_predictions_test = np.zeros((n_test, n_models))\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for i, model in enumerate(base_models):\n",
    "        test_fold_predictions = np.zeros((n_test, n_splits))\n",
    "\n",
    "        for j, (train_idx, valid_idx) in enumerate(kf.split(X_train)):\n",
    "            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n",
    "            y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n",
    "\n",
    "            model.fit(X_tr, y_tr)\n",
    "            base_predictions_train[valid_idx, i] = model.predict(X_val)\n",
    "            test_fold_predictions[:, j] = model.predict(X_test)\n",
    "\n",
    "        base_predictions_test[:, i] = test_fold_predictions.mean(axis=1)\n",
    "\n",
    "    # Train meta model\n",
    "    meta_model.fit(base_predictions_train, y_train)\n",
    "    \n",
    "    # Predict on training set\n",
    "    train_pred = meta_model.predict(base_predictions_train)\n",
    "    final_prediction = meta_model.predict(base_predictions_test)\n",
    "\n",
    "    # Evaluate RMSE\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    for fold, (train_idx, valid_idx) in enumerate(kf.split(base_predictions_train)):\n",
    "        X_val = base_predictions_train[valid_idx]\n",
    "        y_val = y_train.iloc[valid_idx]\n",
    "        val_pred = meta_model.predict(X_val)\n",
    "        score = rmse(y_val, val_pred)\n",
    "        scores.append(score)\n",
    "        print(f\"Fold {fold+1} RMSE: {score:.4f}\")\n",
    "\n",
    "    print(f'Optimized Cross-validated RMSE score: {np.mean(scores):.5f} +/- {np.std(scores):.5f}')\n",
    "    print(f'Max RMSE score: {np.max(scores):.5f}')\n",
    "    print(f'Min RMSE score: {np.min(scores):.5f}')\n",
    "\n",
    "    return final_prediction\n",
    "\n",
    "def score_stacking_Kfold(X, y, X_test, base_models, meta_model):\n",
    "\n",
    "    X = X.copy()\n",
    "    X_test = X_test.copy()\n",
    "\n",
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    test_preds = np.zeros(len(X_test))\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "        print(f\"Training fold {fold + 1}/{n_splits}...\")\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # 1. Buat meta-features untuk training dan validasi\n",
    "        X_train_meta = np.zeros((len(X_train), len(base_models)))\n",
    "        X_val_meta = np.zeros((len(X_val), len(base_models)))\n",
    "        X_test_meta = np.zeros((len(X_test), len(base_models)))\n",
    "\n",
    "        for i, model in enumerate(base_models):\n",
    "            model.fit(X_train, y_train)\n",
    "            X_train_meta[:, i] = model.predict(X_train)\n",
    "            X_val_meta[:, i] = model.predict(X_val)\n",
    "            X_test_meta[:, i] += model.predict(X_test) / n_splits  # Rata-rata nanti\n",
    "\n",
    "        # 2. Train meta-model\n",
    "        meta_model.fit(X_train_meta, y_train)\n",
    "\n",
    "        # 3. Predict dan hitung skor di validation\n",
    "        val_pred = meta_model.predict(X_val_meta)\n",
    "        score = rmse(y_val, val_pred)\n",
    "        scores.append(score)\n",
    "\n",
    "        # 4. Predict untuk test set\n",
    "        test_preds += meta_model.predict(X_test_meta) / n_splits\n",
    "\n",
    "        print(f\"Fold {fold + 1} RMSE: {score:.4f}\")\n",
    "\n",
    "    print(f'Optimized Cross-validated RMSE score: {np.mean(scores):.5f} +/- {np.std(scores):.5f}')\n",
    "    print(f'Max RMSE score: {np.max(scores):.5f}')\n",
    "    print(f'Min RMSE score: {np.min(scores):.5f}')\n",
    "\n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = load_data()\n",
    "X_train, X_test = create_features(df_train, df_test)\n",
    "y_train = df_train.loc[:,'Listening_Time_minutes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "\n",
    "xgb_model = XGBRegressor(\n",
    "    random_state = 0,\n",
    "    n_estimators = 565,\n",
    "    max_depth= 14,\n",
    "    learning_rate = 0.04222221,\n",
    "    subsample= 0.8,\n",
    "    colsample_bytree = 0.8,   \n",
    "    n_jobs= -1 \n",
    ")\n",
    "\n",
    "xgb_model_2 = XGBRegressor(\n",
    "    random_state = 0,\n",
    "    n_estimators = 565,\n",
    "    max_depth=12,\n",
    "    learning_rate=0.05858702616823876,\n",
    "    subsample=0.9356075676850377,\n",
    "    colsample_bytree=0.7895819265828284,\n",
    "    gamma=1.7903575391246762\n",
    ")\n",
    "\n",
    "lgbm_model = LGBMRegressor(\n",
    "    random_state = 0,\n",
    "    n_iter=1000,\n",
    "    max_depth=-1,\n",
    "    num_leaves=1024,\n",
    "    colsample_bytree=0.7,\n",
    "    learning_rate=0.03,\n",
    "    objective='l2',\n",
    "    verbosity=-1,\n",
    "    max_bin=1024,\n",
    ")\n",
    "\n",
    "random_forest_model = RandomForestRegressor(\n",
    "    random_state=0,\n",
    "    n_estimators=500,\n",
    "    max_depth=25,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final Score : {score_dataset(X_train, y_train, model=random_forest_model):.5f} RMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = score_dataset_Kfold(X_train, y_train, X_test, model=xgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without K-fold\n",
    "\n",
    "xgb = xgb_model\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_1 = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With K-Fold\n",
    "\n",
    "xgb = xgb_model\n",
    "y_pred_2 =score_dataset_Kfold(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking Model\n",
    "\n",
    "base_models = [\n",
    "    xgb_model,\n",
    "    random_forest_model,\n",
    "    lgbm_model\n",
    "]\n",
    "\n",
    "meta_model = Ridge(random_state=0)\n",
    "\n",
    "y_pred_3 = stacking_regression_with_rmse(\n",
    "    base_models, \n",
    "    meta_model, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_test,\n",
    "    n_splits=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Stacking Model\n",
    "\n",
    "final_test_prediction = score_stacking_Kfold(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_test, \n",
    "    base_models, \n",
    "    meta_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submisson():\n",
    "    output = pd.DataFrame({'id': X_test.index, 'Listening_Time_minutes': y_pred_1})\n",
    "    output.to_csv('my_submission6.csv', index=False)\n",
    "    print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_submisson()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
